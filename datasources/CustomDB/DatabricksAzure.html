
<!DOCTYPE html>
<html lang="" >
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="book-version"      content=""/>
  <meta name="book-release"      content=""/>
  <meta name="docsearch:name"    content="Alation User Guide" />
  <meta name="docsearch:version" content="" />
  <meta name="docsearch:package_type" content="" />
	<meta name="robots" content="noindex" />

  
  <title>Databricks Azure &mdash; Alation User Guide</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  

  
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/main.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx_collapse.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/rtd_sphinx_search.min.css" type="text/css" />
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Alation User Guide" href="#"/>
        <link rel="up" title="Custom DB" href="index.html"/>
        <link rel="next" title="Azure Databricks: Configure SSO through OAuth for Compose" href="DatabricksAzureOAuthForCompose.html"/>
        <link rel="prev" title="Databricks for Custom DB" href="DatabricksforCustomDB.html"/>

	

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67846571-1"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-67846571-1');
	</script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- Second Google Analytics tag to assist mapping with mattermost.com visits --> 
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-120238482-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-120238482-1');
	</script>


</head>

<body class="wy-body-for-nav" role="document">

  <header>
    <div class="header__container wy-max-content-width">
        <a href="/" class="header__logo">
            <img width="176" src="../../_static/img/logo-alation.svg" alt="Alation" />
        </a>
        <ul class="header__links">
            <li><a href="https://help.alation.com/s/">Alation Help Center</a></li>
        </ul>
    </div>
</header>

  <div class="wy-grid-for-nav wy-max-content-width">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-scroll__content">
          <div class="wy-side-nav-search">
            

            <ul class="sidebartop">
              <li class="nolink search"><div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search this project" aria-label="Search this project" id="searchinput" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
</li>

            </ul>

            
          </div>

          <div class="wy-menu wy-sidebar wy-sidebar--hidden" data-spy="affix" role="navigation" aria-label="main navigation">
            
              
              
              
                  <p class="caption" role="heading"><span class="caption-text">Welcome to Alation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/About/index.html">About Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/CloudAndOnPrem/index.html">Alation Cloud and On-Premise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/UserProfileandPreferences/index.html">User Profile and Preferences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/CatalogBasics/index.html">Catalog Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/CatalogPages/index.html">Catalog Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/Conversations/index.html">Conversations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/Domains/index.html">Domains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/BestPractices/index.html">Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../welcome/Glossary/index.html">Alation Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation Cloud Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cloud/StatusPage/index.html">Status Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cloud/CloudOverview/index.html">Cloud Service Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cloud/CloudConfiguration/index.html">Configuration for Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cloud/AlationAgent/index.html">Alation Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation Integration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../integration/AlationAnywhere/index.html">Alation Anywhere</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../integration/AlationConnectedSheets/index.html">Alation Connected Sheets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation &amp; Configuration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/ServerInstallation/index.html">Install Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/HighAvailability/index.html">High Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/Update/index.html">Update Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/BackupandRestore/index.html">Back Up and Restore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/LineageV2/index.html">Lineage V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/LineageV3/index.html">Lineage V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/AlationAnalyticsV2/index.html">Alation Analytics V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installconfig/AlationContainerService/index.html">Alation Container Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../sources/CatalogSources/index.html">Catalog Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sources/VirtualDataSources/index.html">Virtual Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sources/Lineage/index.html">Lineage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sources/APIResources/index.html">API Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sources/WorkwithCatalogData/index.html">Working with Catalog Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sources/OpenConnectorFramework/index.html">Open Connector Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Sources</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../DSConfiguration/index.html">Data Source Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AddDataSources/index.html">Adding Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ComposeSSOAWSDataSources/index.html">Compose SSO for Amazon Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ComposeSSOAzureDataSources/index.html">Compose SSO for Azure Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AuthenticationForExtraction/index.html">Authentication for Extraction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Custom DB</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="AddCustomDrivers.html">Add Custom Drivers</a></li>
<li class="toctree-l2"><a class="reference internal" href="CDataDrivers.html">CData Drivers</a></li>
<li class="toctree-l2"><a class="reference internal" href="DriverFixer.html">Driver Fixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="AddCustomDBtoAlation.html">Add Custom DB to Alation</a></li>
<li class="toctree-l2"><a class="reference internal" href="MDEFromCustomDB.html">Metadata Extraction From Custom DB</a></li>
<li class="toctree-l2"><a class="reference internal" href="GuidelinesForMDEQueries.html">Guidelines for Creating Queries for MDE</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProfilingForCustomDB.html">Sampling and Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="QLIFromCustomDB.html">QLI From Custom DB</a></li>
<li class="toctree-l2"><a class="reference internal" href="FileBasedQLICustomDB.html">File-Based Query Log Ingestion</a></li>
<li class="toctree-l2"><a class="reference internal" href="ViewBasedQLICustomDB.html">View-Based QLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="SQLQueryBasedQLICustomDB.html">SQL Query QLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="AWSAthena.html">Amazon Athena</a></li>
<li class="toctree-l2"><a class="reference internal" href="UpdateSimbaJDBCDriverForAthenaToSupportSSO.html">Update Simba JDBC Driver for Amazon Athena to Support SSO</a></li>
<li class="toctree-l2"><a class="reference internal" href="AWSDynamoDB.html">Amazon DynamoDB</a></li>
<li class="toctree-l2"><a class="reference internal" href="AzureCosmosDB.html">Azure Cosmos DB</a></li>
<li class="toctree-l2"><a class="reference internal" href="AzureDW.html">Azure Data Warehouse (Azure SQL DW)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ConfluentKafka.html">Confluent Kafka</a></li>
<li class="toctree-l2"><a class="reference internal" href="DatabricksforCustomDB.html">Databricks for Custom DB</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Databricks Azure</a></li>
<li class="toctree-l2"><a class="reference internal" href="DatabricksAzureOAuthForCompose.html">Azure Databricks: Configure SSO through OAuth for Compose</a></li>
<li class="toctree-l2"><a class="reference internal" href="Denodo.html">Denodo</a></li>
<li class="toctree-l2"><a class="reference internal" href="Elasticsearch.html">Elasticsearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="EMRSparkSQL.html">EMR SparkSQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="EMRPrestoWithAWSGlueAsMetastore.html">EMR Presto on Hive with AWS Glue as Metastore</a></li>
<li class="toctree-l2"><a class="reference internal" href="GCPDatabricks.html">GCP Databricks</a></li>
<li class="toctree-l2"><a class="reference internal" href="GitHub.html">GitHub</a></li>
<li class="toctree-l2"><a class="reference internal" href="MariaDB.html">MariaDB</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkLogic.html">MarkLogic</a></li>
<li class="toctree-l2"><a class="reference internal" href="MongoDB.html">MongoDB</a></li>
<li class="toctree-l2"><a class="reference internal" href="MongoDBAtlas.html">MongoDB Atlas</a></li>
<li class="toctree-l2"><a class="reference internal" href="NetSuite.html">NetSuite</a></li>
<li class="toctree-l2"><a class="reference internal" href="Parquet.html">Parquet (Amazon S3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Salesforce.html">Salesforce</a></li>
<li class="toctree-l2"><a class="reference internal" href="Servicenow.html">ServiceNow</a></li>
<li class="toctree-l2"><a class="reference internal" href="Splunk.html">Splunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="SSAS.html">SQL Server Analysis Service (SSAS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="StarburstEnterprisePresto.html">Starburst Enterprise (Presto)</a></li>
<li class="toctree-l2"><a class="reference internal" href="StarburstEnterpriseTrino.html">Starburst Enterprise (Trino)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ReferenceForQueryBasedMDE.html">Example Queries for Query-Based MDE</a></li>
<li class="toctree-l2"><a class="reference internal" href="ReferenceFileBasedQLI.html">Example JSONs for File-Based QLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="ToubleshootingForCDataDataSources.html">Troubleshooting for CData Data Sources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../GBQ/index.html">Google BigQuery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Hive/index.html">Hive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">BI Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bisources/AddBISources/index.html">Add BI Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bisources/AddTableauServer/index.html">Add Tableau Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bisources/WorkwithBISources/index.html">Working with BI Sources</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">File System Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../filesystems/FileSystems/index.html">Add File Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../filesystems/VirtualFileSystems/index.html">Virtual File Systems</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Analysts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analyst/UserAuthenticationForDataSources/index.html">User Authentication for Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analyst/ShareAndAccessQueries/index.html">Share and Access Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analyst/WriteQueries/index.html">Write Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analyst/DevelopQueries/index.html">Develop Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analyst/WorkwithQueryResults/index.html">Work with Query Results</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Data Stewards</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../steward/GovernanceApp/index.html">Governance App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/PolicyCenter/index.html">Policy Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/Workflow/index.html">Workflow Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/GovernanceDashboard/index.html">Governance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/StewardshipWorkbench/index.html">Stewardship Workbench</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/AnalyticsStewardship/index.html">Analytics Stewardship</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/Glossary/index.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/ArticlesAndTags/index.html">Articles and Tags</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/ArticleGroups/index.html">Article Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/DataCatalogCustomization/index.html">Customize Your Data Catalog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/DataDocumentation/index.html">Data Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/AutoTitlingandLexicon/index.html">Auto-titling and Lexicon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../steward/SnowflakeTags/index.html">Snowflake Tags</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Server Administrators</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../admins/CustomizableHomePage/index.html">Customizable Homepage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/AdminSettings/index.html">Administrator Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/AdditionalConfiguration/index.html">Additional Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/RunbookForAdministrators/index.html">Runbook for Administrators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/AlationAPIs/index.html">Alation APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/How-tos/index.html">How-tos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admins/Troubleshooting/index.html">Troubleshooting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Release Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../releases/releasenotes/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/supportmatrices/index.html">Support Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/endofsupport/index.html">End Of Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/snowflakeclladdon/index.html">Snowflake Parser Add-On</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/fieldnotices/index.html">Important Notices and Security Alerts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Archived Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../archive/About/index.html">About the Documentation Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../archive/AdminConfig/index.html">Administration and Update Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../archive/ReleaseNotes/index.html">Release Notes Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../archive/Usage/index.html">Usage and Customization Archive</a></li>
</ul>

              
            
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="wy-nav-top-icon"></i>
        <a href="../../index.html">Alation User Guide</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">

          <div role="navigation" aria-label="breadcrumbs" class="breadcrumbs">
    <a href="../../index.html">Docs</a>
    <span class="separator">&gt;</span>
    <a href="index.html">Custom DB</a>
    <span class="separator">&gt;</span>
    <span>Databricks Azure</span>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody" class="toBeIndexed">
            
  <section id="databricks-azure">
<h1>Databricks Azure<a class="headerlink" href="#databricks-azure" title="Permalink to this headline">¶</a></h1>
<section id="scope-of-support">
<h2>Scope of Support<a class="headerlink" href="#scope-of-support" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Automatic MDE, Sampling and Profiling, file-based QLI, Popularity, Lineage, Compose</p></li>
<li><p><strong>Delta tables</strong> are supported. Alation has the ability to extract Delta tables and supports Metadata Extraction, Profiling, and Compose. Users can write queries against Delta tables in Compose. Delta tables are represented as Table objects in the Alation Catalog.</p></li>
<li><p>Partitioned tables are supported</p></li>
<li><p>For QLI, logs can be located on Azure Blob Storage, ADLS Gen 1 or Gen 2</p></li>
<li><p>From 2020.4, OAuth authentication for Compose: <a class="reference internal" href="DatabricksAzureOAuthForCompose.html"><span class="doc">Azure Databricks: Configure SSO through OAuth for Compose</span></a></p></li>
</ul>
</section>
<section id="preliminaries">
<h2>Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h2>
<p>The following information and configuration is required to configure an Azure Databricks connection in Alation:</p>
<ul>
<li><p><strong>JDBC</strong> driver to connect to the database. Alation has certified the Simba Spark JDBC Driver for Azure Databricks. Refer to the appropriate version of <a class="reference internal" href="../../releases/supportmatrices/index.html"><span class="doc">Support Matrix</span></a> for the driver version.</p>
<blockquote>
<div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>From 2020.4, this driver is available in the list of drivers in Alation. There is no need to add it to the Alation server as a custom driver.</p>
<p>For earlier releases:</p>
<blockquote>
<div><ul class="simple">
<li><p>Driver information and download: <a class="reference external" href="https://www.simba.com/drivers/spark-jdbc-odbc/?gclid=CjwKCAiAiML-BRAAEiwAuWVgguB9Loc-6Shni0AchZAVGnDh6or3aoXMTvIqfVaGhWrDP0ftL3CWHBoC-ywQAvD_BwE">Apache Spark ODBC and JDBC Drivers with SQL Connector</a>.</p></li>
</ul>
</div></blockquote>
</div>
</div></blockquote>
</li>
<li><p><strong>JDBC URI</strong></p>
<blockquote>
<div><ul>
<li><p>Format of the URI for the Simba driver:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="p">:</span><span class="o">//&lt;</span><span class="n">hostname</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">443</span><span class="o">/</span><span class="n">default</span><span class="p">;</span><span class="n">transportMode</span><span class="o">=</span><span class="n">http</span><span class="p">;</span><span class="n">ssl</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span><span class="n">httpPath</span><span class="o">=&lt;</span><span class="n">databricks_http_path_prefix</span><span class="o">&gt;/&lt;</span><span class="n">databricks_cluster_id</span><span class="o">&gt;</span><span class="p">;</span><span class="n">AuthMech</span><span class="o">=</span><span class="mi">3</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Example:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="p">:</span><span class="o">//</span><span class="n">eastus</span><span class="o">.</span><span class="n">azuredatabricks</span><span class="o">.</span><span class="n">net</span><span class="p">:</span><span class="mi">443</span><span class="o">/</span><span class="n">default</span><span class="p">;</span><span class="n">transportMode</span><span class="o">=</span><span class="n">http</span><span class="p">;</span><span class="n">ssl</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span><span class="n">httpPath</span><span class="o">=</span><span class="n">sql</span><span class="o">/</span><span class="n">protocolv1</span><span class="o">/</span><span class="n">o</span><span class="o">/</span><span class="mi">5678080404529670</span><span class="o">/</span><span class="mi">1129</span><span class="o">-</span><span class="mi">091234</span><span class="o">-</span><span class="n">pooh138</span><span class="p">;</span><span class="n">AuthMech</span><span class="o">=</span><span class="mi">3</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Service account</strong> with privileges to access the Azure Databricks cluster for Metadata Extraction and Profiling. Alation recommends to use the following type of authentication:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Token-based Authentication</strong>: For information on how to generate a unique token and use it for authentication, see <a class="reference external" href="https://docs.databricks.com/api/latest/authentication.html#generate-a-token">Authentication using Databricks personal access tokens</a> in Databricks documentation.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Server-side access to  to place the custom driver on the Alation system.</p></li>
<li><p>Additional configuration for QLI on the Databricks side:</p>
<blockquote>
<div><ul class="simple">
<li><p>File-based QLI (<strong>recommended</strong>)</p></li>
<li><p>Table-based QLI</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Pre-configuration for OAuth on the Azure Portal and Databricks side. Please see <a class="reference internal" href="DatabricksAzureOAuthForCompose.html"><span class="doc">Azure Databricks: Configure SSO through OAuth for Compose</span></a>.</p></li>
</ul>
</section>
<section id="service-account">
<h2>Service Account<a class="headerlink" href="#service-account" title="Permalink to this headline">¶</a></h2>
<section id="metadata-extraction">
<h3>Metadata Extraction<a class="headerlink" href="#metadata-extraction" title="Permalink to this headline">¶</a></h3>
<p>The service account must be able to access the tables and metadata to complete the extraction process. Service account requires access to the Databricks cluster.</p>
</section>
<section id="sampling-and-profiling">
<h3>Sampling and Profiling<a class="headerlink" href="#sampling-and-profiling" title="Permalink to this headline">¶</a></h3>
<p>The service account requires access to the Databricks cluster to perform Profiling/Sampling.</p>
</section>
</section>
<section id="steps-in-alation">
<h2>Steps in Alation<a class="headerlink" href="#steps-in-alation" title="Permalink to this headline">¶</a></h2>
<section id="step-1-add-the-jdbc-driver">
<h3>Step 1 - Add the JDBC Driver<a class="headerlink" href="#step-1-add-the-jdbc-driver" title="Permalink to this headline">¶</a></h3>
<p>Alation has certified <strong>Simba Spark JDBC Driver for Azure Databricks</strong> to be used with Azure Databricks data sources. Refer to the appropriate version of <a class="reference internal" href="../../releases/supportmatrices/index.html"><span class="doc">Support Matrix</span></a> for the driver version.</p>
<p><strong>This step only applies to releases below 2020.4</strong>. From 2020.4, the Simba Spark JDBC Driver is available in Alation.</p>
<p>Refer to <a class="reference internal" href="AddCustomDrivers.html"><span class="doc">Add Custom Drivers</span></a> to add the custom driver.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Simba Spark JDBC Driver</strong> for Azure Databricks needs to be fixed with the Driver Fixer tool before it can be used in Alation: <a class="reference internal" href="DriverFixer.html"><span class="doc">Driver Fixer</span></a>.</p>
</div>
</section>
<section id="step-2-set-up-the-connection">
<h3>Step 2  - Set up the Connection<a class="headerlink" href="#step-2-set-up-the-connection" title="Permalink to this headline">¶</a></h3>
<p>Add a new data source and select the <strong>Custom DB</strong> as the  Database Type.
Enter the JDBC URI and select the JDBC driver you have added to Alation (Simba Spark JDBC Driver 2.6.16) from the <strong>Select Driver</strong> drop-down list. Provide other required information and click <strong>Save and Continue</strong>:</p>
<a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure01.png"><img alt="../../_images/DS_DatabricksAzure01.png" class="align-center" src="../../_images/DS_DatabricksAzure01.png" style="width: 3.5in;" /></a>
</section>
<section id="step-3-enter-service-account-credentials">
<h3>Step 3 - Enter Service Account Credentials<a class="headerlink" href="#step-3-enter-service-account-credentials" title="Permalink to this headline">¶</a></h3>
<p>Enter the token information in the <strong>Username</strong> and <strong>Password</strong> fields as follows:</p>
<ul class="simple">
<li><p><strong>Username</strong>: type the word <code class="docutils literal notranslate"><span class="pre">token</span></code>.</p></li>
<li><p><strong>Password</strong>: enter the token string.</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/DS_DatabricksAWS01.png"><img alt="../../_images/DS_DatabricksAWS01.png" class="align-center" src="../../_images/DS_DatabricksAWS01.png" style="width: 3.5in;" /></a>
</section>
<section id="step-4-configure-your-data-source">
<h3>Step 4 - Configure Your Data Source<a class="headerlink" href="#step-4-configure-your-data-source" title="Permalink to this headline">¶</a></h3>
<p>Click <strong>Skip this Step</strong>. QLI for this type of data source is set up on the Settings &gt; <strong>Query Log Ingestion</strong> tab and requires additional configuration on the Databricks side.</p>
<p>After this step you are navigated to the <strong>Settings</strong> page of your data source.</p>
</section>
</section>
<section id="oauth-for-compose">
<h2>OAuth for Compose<a class="headerlink" href="#oauth-for-compose" title="Permalink to this headline">¶</a></h2>
<p>You can configure OAuth for Compose on the <strong>General Settings</strong> tab. For details, see <a class="reference internal" href="DatabricksAzureOAuthForCompose.html"><span class="doc">Azure Databricks: Configure SSO through OAuth for Compose</span></a>.</p>
</section>
<section id="id1">
<h2>Metadata Extraction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Before running MDE, set the <strong>Catalog Objects Definition</strong> (available from version 2020.3). Without it, the Simba driver adds the <code class="docutils literal notranslate"><span class="pre">spark.</span></code> prefix to extracted metadata objects: <a class="reference internal" href="MDEFromCustomDB.html#metadata-extraction-from-custom-db-set-the-catalog-object-definition"><span class="std std-ref">Set the Catalog Object Definition</span></a>.</p>
<p>To set the Catalog Object Definition,</p>
<ol class="arabic simple">
<li><p>On the Settings page, open the <strong>Custom Settings</strong> tab.</p></li>
<li><p>For <strong>Catalog Object Definition</strong>, select <strong>Schema.Table</strong>.</p></li>
</ol>
<a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure07.png"><img alt="../../_images/DS_DatabricksAzure07.png" class="align-center" src="../../_images/DS_DatabricksAzure07.png" style="width: 3.5in;" /></a>
<ol class="arabic simple">
<li><p>Open the <strong>Metadata Extraction</strong> tab of the settings to configure and perform MDE.</p></li>
</ol>
<p>For Azure Databricks, Alation supports automatic MDE, manual or scheduled. Custom query-based MDE is not supported for this type of data source.</p>
<p>To configure and run MDE,</p>
<p>On the <strong>Metadata Extraction</strong> tab, select or exclude schemas to be extracted, and then run MDE manually or set a schedule. See <a class="reference internal" href="MDEFromCustomDB.html#metadata-extraction-from-custom-db-perform-mde"><span class="std std-ref">Perform MDE</span></a> for details.</p>
</section>
<section id="id2">
<h2>Sampling and Profiling<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>To configure and run Profiling for this type of data source, refer to <a class="reference internal" href="ProfilingForCustomDB.html"><span class="doc">Sampling and Profiling</span></a>.</p></li>
<li><p>Before you run Profiling, consider setting the <strong>Limit Query Template</strong> (available from 2020.3). See <a class="reference internal" href="ProfilingForCustomDB.html#sampling-profiling-limit-query-template"><span class="std std-ref">Custom Settings &gt; Limit Query Template</span></a>. For Azure Databricks, this is: <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">COLUMNS</span> <span class="pre">FROM</span> <span class="pre">TABLE_NAME</span> <span class="pre">FILTER_CLAUSES</span> <span class="pre">LIMIT</span> <span class="pre">ROW_SIZE</span></code> .</p></li>
</ul>
</section>
<section id="steps-to-configure-query-log-ingestion">
<h2>Steps to Configure Query Log Ingestion<a class="headerlink" href="#steps-to-configure-query-log-ingestion" title="Permalink to this headline">¶</a></h2>
<section id="file-based-qli">
<h3>File Based QLI<a class="headerlink" href="#file-based-qli" title="Permalink to this headline">¶</a></h3>
<p>This is the recommended approach. For general information about file-based QLI, see <a class="reference internal" href="FileBasedQLICustomDB.html"><span class="doc">File-Based Query Log Ingestion</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>QLI is supported for Standard and High Concurrency Databricks clusters.</p>
</div>
<p>QLI configuration requires that you enable logging on your Databricks Cluster. There are several ways to enable logs for QLI. You can either add a Python <strong>init</strong> script to your Cluster to enable logs (<strong>recommended</strong>) or use a script to enable logs and run it every time the Cluster is started or restarted</p>
<p>Alation recommends to use the <strong>Python init</strong> script method (certified by Alation from version 2021.2 ) as this method generates log files of smaller size and the admin does not need to run the script again when the Cluster is restarted.</p>
<section id="enable-logs-with-an-init-script">
<h4>Enable Logs with an Init Script<a class="headerlink" href="#enable-logs-with-an-init-script" title="Permalink to this headline">¶</a></h4>
<p><em>Applies from 2021.2</em></p>
<p>This is the recommended option. Perform the steps in this sections to enable QLI using the Python init script:</p>
<blockquote>
<div><ol class="arabic">
<li><p>In the Databricks settings portal, ensure that the <strong>Cluster Log Path</strong> and <strong>Destination</strong> are set under the <strong>Logging tab</strong>. Do NOT leave the Destination path as None.</p>
<blockquote>
<div><a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure03.png"><img alt="../../_images/DS_DatabricksAzure03.png" class="align-center" src="../../_images/DS_DatabricksAzure03.png" style="width: 5.5in;" /></a>
</div></blockquote>
</li>
</ol>
<blockquote>
<div><p>For more information on how to set the logging path in DBFS, see <a class="reference external" href="https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#--cluster-event-logs">MS Documentation</a>.
You need to mount external storage onto the DBFS for log storage. For more information on the mount process, see:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs">Azure Blob documentation</a></p></li>
<li><p><a class="reference external" href="https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html">ADLS Gen 2</a></p></li>
</ul>
</div></blockquote>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Create a  <a class="reference external" href="https://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage#create-a-notebook">Python Notebook</a> and run the following script on your Databricks Cluster using this Python notebook. This script creates the <strong>scripts</strong> directory where the QLI script will be stored:</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.mkdirs<span class="o">(</span><span class="s2">&quot;dbfs:/databricks/scripts/&quot;</span><span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>In the same notebook, run the script given below to create the file with the init script in the <strong>scripts</strong> directory.</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.put<span class="o">(</span><span class="s2">&quot;/databricks/scripts/init.sh&quot;</span>,<span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#!/bin/bash</span>
<span class="s2">echo &quot;</span>Executing<span class="w"> </span>on<span class="w"> </span>Driver:<span class="w"> </span><span class="nv">$DB_IS_DRIVER</span><span class="s2">&quot;</span>
<span class="s2">if [[ </span><span class="nv">$DB_IS_DRIVER</span><span class="s2"> = &quot;</span>TRUE<span class="s2">&quot; ]]; then</span>
<span class="s2">LOG4J_PATH=&quot;</span>/home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties<span class="s2">&quot;</span>
<span class="s2">else</span>
<span class="s2">LOG4J_PATH=&quot;</span>/home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties<span class="s2">&quot;</span>
<span class="s2">fi</span>
<span class="s2">echo &quot;</span>Adjusting<span class="w"> </span>log4j.properties<span class="w"> </span>here:<span class="w"> </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="s2">echo &quot;</span>log4j.logger.org.apache.spark.sql.execution.SparkSqlParser<span class="o">=</span>DEBUG<span class="s2">&quot; &gt;&gt; </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span>
<span class="s2">echo &quot;</span>log4j.appender.publicFile.layout.ConversionPattern<span class="o">=</span>%d<span class="o">{</span>yyyy-MM-dd<span class="w"> </span>HH:mm:ss.SS<span class="o">}</span><span class="w"> </span><span class="o">[</span>%t<span class="o">]</span><span class="w"> </span>%p<span class="w"> </span>%c<span class="o">{</span><span class="m">1</span><span class="o">}</span>:<span class="w"> </span>%m%n<span class="s2">&quot; &gt;&gt; </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>,<span class="w"> </span>True<span class="o">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Alternatively, you can create the init script given below locally and copy it to the Databricks cluster using the following command:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbfs<span class="w"> </span>cp<span class="w"> </span>init.sh<span class="w"> </span>dbfs:/databricks/scripts/init.sh
</pre></div>
</div>
</div>
</div></blockquote>
<ol class="arabic" start="4">
<li><p>Use the following command to make sure that the script was created successfully:</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>display<span class="o">(</span>dbutils.fs.ls<span class="o">(</span><span class="s2">&quot;dbfs:/databricks/scripts/init.sh&quot;</span><span class="o">))</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Use the cluster configuration page to configure the cluster to run the <strong>init</strong> script: <a class="reference external" href="https://docs.microsoft.com/en-us/azure/databricks/clusters/init-scripts#configure-a-cluster-scoped-init-script-using-the-ui">Add init script</a>.</p></li>
<li><p>Restart the cluster.</p></li>
</ol>
</div></blockquote>
</section>
<section id="enable-logs-with-a-python-script">
<h4>Enable Logs With a Python Script<a class="headerlink" href="#enable-logs-with-a-python-script" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li><p>In the Databricks settings portal, ensure that the cluster log path and destination is set under the <strong>Logging</strong> tab. Do NOT set the DESTINATION PATH as NONE.</p></li>
</ol>
<blockquote>
<div><a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure03.png"><img alt="../../_images/DS_DatabricksAzure03.png" class="align-center" src="../../_images/DS_DatabricksAzure03.png" style="width: 5.5in;" /></a>
<ul>
<li><p>For more information on how to set the logging path in DBFS, see <a class="reference external" href="https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#--cluster-event-logs">the corresponding Databricks documentation</a>.</p></li>
<li><p>You can choose to mount external storage onto the DBFS for log storage. For more information on mount process, see:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://docs.databricks.com/data/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs">Azure Blob</a></p></li>
<li><p><a class="reference external" href="https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html">ADLS Gen 2</a></p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Open a Databricks Python notebook. For more information on creating a notebook, see <a class="reference external" href="https://docs.databricks.com/user-guide/notebooks/notebook-manage.html#create-a-notebook">Create a Notebook</a></p></li>
</ol>
<blockquote>
<div><a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure04.png"><img alt="../../_images/DS_DatabricksAzure04.png" class="align-center" src="../../_images/DS_DatabricksAzure04.png" style="width: 5.5in;" /></a>
</div></blockquote>
<ol class="arabic" start="3">
<li><p>Copy the contents of the Python script below to enable DEBUG query logs that contain SQL queries and set the pattern in which logs have to show up. This Python script should be run whenever the cluster gets started or restarted.</p>
<blockquote>
<div><p><strong>Python script to enable DEBUG query logs</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log4j</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">log4j</span>
<span class="n">log4j</span><span class="o">.</span><span class="n">LogManager</span><span class="o">.</span><span class="n">getRootLogger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">log4j</span><span class="o">.</span><span class="n">Level</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">ca</span> <span class="o">=</span> <span class="n">log4j</span><span class="o">.</span><span class="n">LogManager</span><span class="o">.</span><span class="n">getRootLogger</span><span class="p">()</span><span class="o">.</span><span class="n">getAppender</span><span class="p">(</span><span class="s2">&quot;publicFile&quot;</span><span class="p">)</span>
<span class="n">ca</span><span class="o">.</span><span class="n">setLayout</span><span class="p">(</span><span class="n">log4j</span><span class="o">.</span><span class="n">PatternLayout</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">{yyyy-MM-dd HH:mm:ss.SS} [%t] %p </span><span class="si">%c{1}</span><span class="s2">: %m%n&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</section>
</section>
<section id="set-up-file-based-qli-in-alation">
<h3>Set Up File-Based QLI in Alation<a class="headerlink" href="#set-up-file-based-qli-in-alation" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p>On the Query Log Ingestion tab, clear the Enable table based query log ingestion checkbox. This reveals the field named Log Extraction Configuration Json.</p>
<blockquote>
<div><a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure06.png"><img alt="../../_images/DS_DatabricksAzure06.png" class="align-center" src="../../_images/DS_DatabricksAzure06.png" style="width: 5.5in;" /></a>
</div></blockquote>
</li>
<li><p>Paste the JSON given below in the <strong>Log Extraction Configuration</strong> Json field.</p></li>
<li><p>In the JSON, change the folder path to the path of the storage mount where the generated logs (the <strong>Logging Path</strong> you have configured under <strong>Logging</strong> on the Databricks side) are stored and click <strong>Save</strong>.</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="s2">&quot;folderPath&quot;</span>:<span class="s2">&quot;/path/to/log/file/&quot;</span>,
<span class="s2">&quot;nThread&quot;</span>:<span class="s2">&quot;10&quot;</span>,
<span class="s2">&quot;threadTimeOut&quot;</span>:<span class="s2">&quot;2000&quot;</span>,
<span class="s2">&quot;parserType&quot;</span>:<span class="s2">&quot;LOG4J&quot;</span>,
<span class="s2">&quot;log4jConversionPattern&quot;</span>:<span class="s2">&quot;TIMESTAMP [THREAD] LEVEL LOGGER MESSAGE&quot;</span>,
<span class="s2">&quot;log4jTimeFormat&quot;</span>:<span class="s2">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>,
<span class="s2">&quot;requiredExtraction&quot;</span>:<span class="o">[</span>
<span class="o">{</span>
<span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractSqlQuery&quot;</span>,
<span class="s2">&quot;keyValuePair&quot;</span>:<span class="o">{</span>
<span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;SparkSqlParser&quot;</span>,
<span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;Parsing command:(?&lt;queryString&gt;[\\w\\W]*)&quot;</span>
<span class="o">}</span>
<span class="o">}</span>,
<span class="o">{</span>
<span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractUserInfo&quot;</span>,
<span class="s2">&quot;keyValuePair&quot;</span>:<span class="o">{</span>
<span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;audit&quot;</span>,
<span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;ugi=(?:\\(Basic token\\))?(?&lt;userName&gt;[\\S]+)&quot;</span>
<span class="o">}</span>
<span class="o">}</span>,
<span class="o">{</span>
<span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractTimeTaken&quot;</span>,
<span class="s2">&quot;keyValuePair&quot;</span>:<span class="o">{</span>
<span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;Retrieve&quot;</span>,
<span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;Execution Time = (?&lt;milliSeconds&gt;[\\d]+)&quot;</span>
<span class="o">}</span>
<span class="o">}</span>
<span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="4">
<li><p>Select the relevant Connection Type from the following list:</p>
<blockquote>
<div><ul class="simple">
<li><p>Amazon S3</p></li>
<li><p>Azure Blob Storage</p></li>
<li><p>ADLS</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Provide the Connection configuration details based on the Connection Type and click <strong>Save Configuration</strong>. The Connection information is required for Alation to read the logs from the Logging Path:</p></li>
</ol>
<table class="docutils align-default">
<colgroup>
<col style="width: 39%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Connection Type</p></th>
<th class="head"><p>Configuration Details</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Amazon S3</p></td>
<td><ul class="simple">
<li><p>AWS Access Key ID</p></li>
<li><p>AWS Access Key Secret</p></li>
<li><p>AWS Region</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Azure Blob Storage</p></td>
<td><ul class="simple">
<li><p>Storage Account Name</p></li>
<li><p>Access Key/Shared Access Signature</p></li>
<li><p>Blob Container</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>ADLS</p></td>
<td><ul class="simple">
<li><p>ADLS URI</p></li>
<li><p>Tenant ID</p></li>
<li><p>Client ID</p></li>
<li><p>Client Secret</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<ol class="arabic simple" start="6">
<li><p>Under <strong>Run Manual Query Ingestion</strong>, select the <strong>Date Range</strong>.</p></li>
<li><p>Click <strong>Preview</strong> to preview the logs to be imported.</p></li>
<li><p>Click <strong>Import</strong> to perform QLI. The status of the QLI job will be reflected in the <strong>Job History</strong> table at the bottom of the page.</p></li>
</ol>
</section>
<section id="table-based-qli">
<h3>Table-Based QLI<a class="headerlink" href="#table-based-qli" title="Permalink to this headline">¶</a></h3>
<p>QLI can be supported using the External table. For more information on external tables,
see <a class="reference external" href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html#create-table">this article</a>.</p>
<section id="steps-to-be-done-on-the-databricks-side">
<h4>Steps to be done on the Databricks side:<a class="headerlink" href="#steps-to-be-done-on-the-databricks-side" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p>After you have enabled the DEBUG-level logs as described in <a class="reference internal" href="#enable-logs-with-a-python-script">Enable Logs With a Python Script</a>, create the logs directory in the cluster path. Create the required folder before running the next Python script. If this is not created, “File Not Found” error will occur.</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.mkdirs<span class="o">(</span><span class="s2">&quot;dbfs:/cluster-logs/&lt;cluster-id&gt;/logs/&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.mkdirs<span class="o">(</span><span class="s2">&quot;dbfs:/cluster-logs/0130-102557-aft119/logs/&quot;</span><span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<ol class="arabic" start="2">
<li><p>Open a new Databricks Python notebook.</p></li>
<li><p>Copy the contents from the Python script below into a Python notebook cell. Replace the <code class="docutils literal notranslate"><span class="pre">input_dir</span></code> and <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> in the script with actual values.</p>
<blockquote>
<div><ul>
<li><p>If you are accessing the directories mounted in the Databricks File System (DBFS), use the prefix <code class="docutils literal notranslate"><span class="pre">/dbfs/mnt/&lt;mounted_filepath&gt;</span></code>.</p>
<blockquote>
<div><p><strong>Example</strong>: <code class="docutils literal notranslate"><span class="pre">/dbfs/mnt/cluster-logs/0130-102557-aft119/driver/</span></code> where <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs</span></code> is the mounted path to a Blob file system</p>
</div></blockquote>
</li>
<li><p>If you are accessing the log files that are directly in DBFS, use the prefix <code class="docutils literal notranslate"><span class="pre">/dbfs/&lt;filepath&gt;</span></code></p>
<blockquote>
<div><p><strong>Example</strong>: <code class="docutils literal notranslate"><span class="pre">/dbfs/0206-072111-lox669/driver/</span></code>.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ol>
<p><strong>Python Script to enable QLI</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>


<span class="n">input_dir</span> <span class="o">=</span> <span class="s1">&#39;/dbfs/mnt/cluster-logs/0130-102557-aft119/driver/&#39;</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;/dbfs/mnt/cluster-logs/0130-102557-aft119/logs/&#39;</span>

<span class="n">required_logger_line_regex</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;^\d+-\d+-\d+ \d+:\d+:\d+\.\d* \[[^\]]*?\] \S+ (?:SparkSqlParser|audit): .*&#39;</span>
<span class="n">logger_line_regex</span> <span class="o">=</span>  <span class="sa">r</span><span class="s1">&#39;^\d+-\d+-\d+ \d+:\d+:\d+\.\d* \[[^\]]*?\] \S+ \S+:.*&#39;</span>

<span class="n">prev_line</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="n">line_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">file_date_hour</span> <span class="o">=</span> <span class="nb">str</span><span class="p">((</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hours</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">-%H&#39;</span><span class="p">))</span>

<span class="n">input_file_name</span> <span class="o">=</span> <span class="n">input_dir</span> <span class="o">+</span> <span class="s1">&#39;log4j-&#39;</span> <span class="o">+</span> <span class="n">file_date_hour</span> <span class="o">+</span> <span class="s1">&#39;.log.gz&#39;</span>
<span class="n">output_file_name</span> <span class="o">=</span> <span class="n">output_dir</span> <span class="o">+</span> <span class="s1">&#39;log4j-&#39;</span> <span class="o">+</span> <span class="n">file_date_hour</span> <span class="o">+</span> <span class="s1">&#39;.log.gz&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Transforming log4j-</span><span class="si">{}</span><span class="s1">.log.gz&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_date_hour</span><span class="p">))</span>

<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">input_file_name</span><span class="p">,</span> <span class="s1">&#39;rt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">output_file_name</span><span class="p">,</span> <span class="s1">&#39;wt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fin</span><span class="p">:</span>
      <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">required_logger_line_regex</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
          <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
          <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">prev_line</span> <span class="o">=</span> <span class="n">line</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">logger_line_regex</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">prev_line</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">required_logger_line_regex</span><span class="p">,</span> <span class="n">prev_line</span><span class="p">):</span>
          <span class="n">prev_line</span> <span class="o">=</span> <span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">line</span>
    <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
      <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Lines written: &#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">line_count</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Specify the correct file path for input dir and output dir. If the file path for input dir or output dir is not specified properly, the script will not be able to find the files.</p>
</div>
<ol class="arabic" start="4">
<li><p>Run the script after the previous hour log file is created and verify that you get the expected output. The expected output when the previous hour file name is <strong>log4j-2019-05-02-10.log.gz</strong> and when there is no query or user information in the file is as follows:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Transforming</span> <span class="n">log4j</span><span class="o">-</span><span class="mi">2019</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mf">10.</span><span class="n">log</span><span class="o">.</span><span class="n">gz</span>
<span class="n">Lines</span> <span class="n">written</span><span class="p">:</span>  <span class="mi">0</span>
</pre></div>
</div>
<p>The expected output when the previous hour file name is <strong>log4j-2019-05-02-11.log.gz</strong> and when there is query or user information is available in the file is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Transforming</span> <span class="n">log4j</span><span class="o">-</span><span class="mi">2019</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mf">11.</span><span class="n">log</span><span class="o">.</span><span class="n">gz</span>
<span class="n">Lines</span> <span class="n">written</span><span class="p">:</span>  <span class="mi">275</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Schedule the QLI Python script to run at or after 10 minutes past the hour. If the Python script is scheduled exactly at the hour, the log file will not be available. There will be a five minute delay in writing the contents to the log file and converting the log file into a compressed <strong>gzip</strong> file. So it is advisable to schedule the Python script to run at or after 10 minutes past the hour, which ensures the availability of log files.</p></li>
<li><p>Create an External Table with the location property set to the directory where the new files are stored.</p>
<blockquote>
<div><ul class="simple">
<li><p>For setting the Location property to the directories mounted in DBFS, use the prefix <code class="docutils literal notranslate"><span class="pre">/mnt/&lt;mounted_filepath&gt;</span></code></p></li>
</ul>
<p><strong>Example</strong>: <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs/0130-102557-aft119/logs/</span></code> where <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs</span></code> is the mounted path to a Blob file system.</p>
<ul class="simple">
<li><p>For setting the Location property to the directory present in DBFS, use the prefix <code class="docutils literal notranslate"><span class="pre">/&lt;filepath&gt;</span></code></p></li>
</ul>
<p><strong>Example</strong>: <code class="docutils literal notranslate"><span class="pre">/0206-072111-lox669/logs/</span></code></p>
</div></blockquote>
</li>
</ol>
<p><strong>Sample External Table Creation Query</strong>:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">EXTERNAL</span><span class="w"> </span><span class="k">TABLE</span>
<span class="w">  </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="p">(</span>
<span class="w">    </span><span class="n">date_time_string</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">thread_name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="k">level</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">logger</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">    </span><span class="n">message</span><span class="w"> </span><span class="n">STRING</span>
<span class="w">   </span><span class="p">)</span>
<span class="w"> </span><span class="k">ROW</span><span class="w"> </span><span class="n">FORMAT</span>
<span class="w">   </span><span class="n">SERDE</span><span class="w"> </span><span class="s1">&#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;</span>
<span class="w"> </span><span class="k">WITH</span><span class="w"> </span><span class="n">SERDEPROPERTIES</span>
<span class="w">   </span><span class="p">(</span><span class="ss">&quot;input.regex&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;^(\\S+ \\S+) \\[(.*?)\\] (\\S+) (\\S+): (.*?)&quot;</span><span class="p">)</span>
<span class="w"> </span><span class="k">LOCATION</span>
<span class="w">   </span><span class="ss">&quot;/mnt/cluster-logs/0130-102557-aft119/logs/&quot;</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li><p>Ensure that the external table is populated with the data from the files stored in the <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> by running a select query. This step ascertains that the external table is populated with data from files stored in the <code class="docutils literal notranslate"><span class="pre">output_dir</span></code>. Example: <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">*</span> <span class="pre">FROM&lt;SCHEMA.EXTERNAL_TABLE_NAME&gt;</span></code></p></li>
<li><p>Create the  <strong>alation_qli</strong> view which takes this external table as an input and gives the data as required by Alation.</p></li>
</ol>
<p><strong>View Query for QLI</strong>:</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">VIEW</span>
<span class="w">  </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span>
<span class="k">AS</span>
<span class="k">SELECT</span>
<span class="w">  </span><span class="k">distinct</span><span class="w"> </span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="n">CONCAT</span><span class="p">(</span><span class="n">userName</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">startTime</span><span class="p">)</span><span class="w"> </span><span class="n">sessionId</span>
<span class="k">FROM</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="k">SELECT</span>
<span class="w">      </span><span class="n">a</span><span class="p">.</span><span class="n">date_time_string</span><span class="w"> </span><span class="n">startTime</span><span class="p">,</span>
<span class="w">      </span><span class="n">regexp_extract</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;Parsing command: (.*)&quot;</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">queryString</span><span class="p">,</span>
<span class="w">      </span><span class="k">CASE</span>
<span class="w">        </span><span class="k">WHEN</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">null</span><span class="w"> </span><span class="k">THEN</span><span class="w"> </span><span class="s1">&#39;unknown&#39;</span>
<span class="w">        </span><span class="k">WHEN</span><span class="w"> </span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;Basic token&#39;</span><span class="w"> </span><span class="k">THEN</span><span class="w"> </span><span class="k">TRIM</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">18</span><span class="p">,</span><span class="w"> </span><span class="n">instr</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ip=&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">19</span><span class="p">))</span>
<span class="w">        </span><span class="k">ELSE</span><span class="w"> </span><span class="k">TRIM</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="n">instr</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ip=&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">6</span><span class="p">))</span>
<span class="w">      </span><span class="k">END</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">userName</span>
<span class="w">    </span><span class="k">FROM</span>
<span class="w">      </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="w"> </span><span class="n">a</span>
<span class="w">    </span><span class="k">LEFT</span><span class="w"> </span><span class="k">OUTER</span><span class="w"> </span><span class="k">JOIN</span>
<span class="w">      </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">thread_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">thread_name</span><span class="p">)</span>
<span class="w">    </span><span class="k">WHERE</span>
<span class="w">      </span><span class="n">a</span><span class="p">.</span><span class="n">logger</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;SparkSqlParser&#39;</span>
<span class="w">      </span><span class="k">and</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">logger</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;audit&#39;</span>
<span class="w">      </span><span class="k">and</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">date_time_string</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">date_time_string</span>
<span class="w">  </span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="steps-to-be-done-on-the-alation-side">
<h4>Steps to be done on the Alation side:<a class="headerlink" href="#steps-to-be-done-on-the-alation-side" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li><p>Go to your Databricks data source Settings page &gt; <strong>Query Log Ingestion</strong> tab.</p></li>
</ol>
<ol class="arabic" start="2">
<li><p>Select the checkbox <strong>Enable table based query log ingestion</strong>. This reveals a field named <strong>Query to Execute</strong>.</p></li>
<li><p>Enter the following query to enable table-based QLI for Databricks. Do NOT substitute values <code class="docutils literal notranslate"><span class="pre">STARTTIME1</span></code> and <code class="docutils literal notranslate"><span class="pre">STARTTIME2</span></code>; use them as is.</p>
<blockquote>
<div><p><strong>Final Query</strong>:</p>
<blockquote>
<div><div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span>
<span class="w">  </span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">startTime</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">19</span><span class="p">)</span><span class="w"> </span><span class="n">startTime</span><span class="p">,</span>
<span class="w">  </span><span class="n">queryString</span><span class="p">,</span>
<span class="w">  </span><span class="n">userName</span><span class="p">,</span>
<span class="w">  </span><span class="n">sessionID</span>
<span class="k">FROM</span>
<span class="w">  </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span>
<span class="k">WHERE</span>
<span class="w">  </span><span class="n">startTime</span><span class="w"> </span><span class="k">between</span><span class="w"> </span><span class="s1">&#39;STARTTIME1&#39;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="s1">&#39;STARTTIME2&#39;</span>
</pre></div>
</div>
</div></blockquote>
<a class="reference internal image-reference" href="../../_images/pasted_image_dbricks.png"><img alt="../../_images/pasted_image_dbricks.png" src="../../_images/pasted_image_dbricks.png" style="width: 5.5in;" /></a>
</div></blockquote>
</li>
</ol>
</section>
</section>
</section>
<section id="troubleshooting-qli">
<h2>Troubleshooting QLI<a class="headerlink" href="#troubleshooting-qli" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>The query log file should be in a compressed file format <strong>.gz</strong>. If the file exists in a different format, the script throws a <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">not</span> <span class="pre">found</span></code> error.</p></li>
<li><p>Ensure the file name adheres to the <strong>log4j-yyyy-mm-dd-HH.log.gz</strong> format. Example: <strong>log4j-2019-03-27-01.log.gz</strong>. If the file exists in a format other than the specified format, the script will not pick up the file for execution.</p></li>
<li><p>If the input directory or output directory is not specified properly, the script will be executed, but throws an error:</p></li>
</ol>
<blockquote>
<div><a class="reference internal image-reference" href="../../_images/DS_DatabricksAzure05.png"><img alt="../../_images/DS_DatabricksAzure05.png" src="../../_images/DS_DatabricksAzure05.png" style="width: 5.5in;" /></a>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>If the path is valid and the file is not found, the error message <strong>File not found in the path</strong> also occurs.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer>

  
<div class="navigation-bar">

    <div class="navigation navigation--prev">
        
        <a class="navigation__link" href="DatabricksforCustomDB.html">
            <button class="navigation__button">
                <i class="icon icon-left"></i>
            </button>
            <div class="navigation__title">
                <span class="heading">Previous</span> <br />Databricks for Custom DB
            </div>
        </a>
        
    </div>

    <div class="navigation navigation--next">
        
        <a class="navigation__link" title="Accesskey Alt(+Shift)+p" accesskey="p" href="DatabricksAzureOAuthForCompose.html">
            <div class="navigation__title">
                <span class="heading">Next</span> <br />Azure Databricks: Configure SSO through OAuth for Compose
            </div>
            <button class="navigation__button" title="Accesskey Alt(+Shift)+n" accesskey="n">
                <i class="icon icon-right"></i>
            </button>
        </a>
        
    </div>

</div>


</footer>

        </div>
      </div>

    </section>

    <div class="wy-nav-side wy-nav-secondary">
      <div class="wy-side-scroll">
        <div class="wy-side-scroll__content">
          <div class="wy-menu-secondary">
              <div class="comments">
    <a href="mailto:no-reply-docs-feedback@alation.com?subject=Documentation Feedback on datasources/CustomDB/DatabricksAzure&body=Alation%20welcomes%20your%20comments%20on%20typos,%20content%20accuracy%20and%20completeness,%20and%20whether%20you%20found%20this%20page%20helpful.%20You%20will%20not%20receive%20a%20reply%20to%20this%20email.">
        <i class="icon icon-envelope" aria-hidden="true"></i>Request docs changes
    </a>
    <a href="#open-modal-pdf">
        <i class="icon icon-pdf" aria-hidden="true"></i>Download PDF
    </a>
</div>
              
                <p class="wy-menu-secondary-header">On this page</p>
                <ul>
<li><a class="reference internal" href="#">Databricks Azure</a><ul>
<li><a class="reference internal" href="#scope-of-support">Scope of Support</a></li>
<li><a class="reference internal" href="#preliminaries">Preliminaries</a></li>
<li><a class="reference internal" href="#service-account">Service Account</a><ul>
<li><a class="reference internal" href="#metadata-extraction">Metadata Extraction</a></li>
<li><a class="reference internal" href="#sampling-and-profiling">Sampling and Profiling</a></li>
</ul>
</li>
<li><a class="reference internal" href="#steps-in-alation">Steps in Alation</a><ul>
<li><a class="reference internal" href="#step-1-add-the-jdbc-driver">Step 1 - Add the JDBC Driver</a></li>
<li><a class="reference internal" href="#step-2-set-up-the-connection">Step 2  - Set up the Connection</a></li>
<li><a class="reference internal" href="#step-3-enter-service-account-credentials">Step 3 - Enter Service Account Credentials</a></li>
<li><a class="reference internal" href="#step-4-configure-your-data-source">Step 4 - Configure Your Data Source</a></li>
</ul>
</li>
<li><a class="reference internal" href="#oauth-for-compose">OAuth for Compose</a></li>
<li><a class="reference internal" href="#id1">Metadata Extraction</a></li>
<li><a class="reference internal" href="#id2">Sampling and Profiling</a></li>
<li><a class="reference internal" href="#steps-to-configure-query-log-ingestion">Steps to Configure Query Log Ingestion</a><ul>
<li><a class="reference internal" href="#file-based-qli">File Based QLI</a><ul>
<li><a class="reference internal" href="#enable-logs-with-an-init-script">Enable Logs with an Init Script</a></li>
<li><a class="reference internal" href="#enable-logs-with-a-python-script">Enable Logs With a Python Script</a></li>
</ul>
</li>
<li><a class="reference internal" href="#set-up-file-based-qli-in-alation">Set Up File-Based QLI in Alation</a></li>
<li><a class="reference internal" href="#table-based-qli">Table-Based QLI</a><ul>
<li><a class="reference internal" href="#steps-to-be-done-on-the-databricks-side">Steps to be done on the Databricks side:</a></li>
<li><a class="reference internal" href="#steps-to-be-done-on-the-alation-side">Steps to be done on the Alation side:</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting-qli">Troubleshooting QLI</a></li>
</ul>
</li>
</ul>

              
            </div>
          </div>
      </div>
    </div>

  </div>

  
  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->

  
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/js/rtd_sphinx_search.min.js"></script>
  
  <script type="text/javascript" src="../../_static/searchtools.js"></script> <script type="text/javascript" id="idPiwikScriptPlaceholder"></script><script type="text/javascript" src="../../_static/js/main.bundle.js"></script>
  <script type="text/javascript" src="../../_static/js/runtime.bundle.js"></script>

  
  

  
    <div id="open-modal-pdf" class="modal-window">
<div>
  <a href="#" title="Close" class="modal-close">Close</a>
  <h4>Alation Documentation PDF</h4>
  <p>This PDF contains all our documentation and is more than <b>120 MB</b> in size. We recommend using an ethernet or Wi-Fi connection for downloading.</p>
  <br>
  <div class="modal-buttons">
    <a href="https://pdf-docs.alation.com/AlationUserDocs.pdf" class="btn-cta btn-cta--primary" target="_blank">
      Download PDF
    </a>
    <a href="#" title="Close" class="btn-cta">Cancel</a>
  </div>
</div>
  

</body>
</html>
<!--3.6.17-->