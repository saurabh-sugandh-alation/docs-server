
<!DOCTYPE html>
<html lang="" >
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="book-version"      content=""/>
  <meta name="book-release"      content=""/>
  <meta name="docsearch:name"    content="Alation User Guide" />
  <meta name="docsearch:version" content="" />
  <meta name="docsearch:package_type" content="" />
	<meta name="robots" content="noindex" />

  
  <title>Azure Databricks OCF Connector: Query Log Ingestion &mdash; Alation User Guide</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  

  

  
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/main.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_collapse.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/rtd_sphinx_search.min.css" type="text/css" />
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="Alation User Guide" href="#"/>
        <link rel="up" title="Azure Databricks OCF Connector" href="index.html"/>
        <link rel="next" title="Azure SQL DB OCF Connector" href="../AzureSQLDBOCFConnector/index.html"/>
        <link rel="prev" title="Azure Databricks OCF Connector: Install and Configure" href="AzureDatabricksOCFConnectorInstallandConfigure.html"/>

	

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67846571-1"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-67846571-1');
	</script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- Second Google Analytics tag to assist mapping with mattermost.com visits --> 
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-120238482-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-120238482-1');
	</script>


</head>

<body class="wy-body-for-nav" role="document">

  <header>
    <div class="header__container wy-max-content-width">
        <a href="/" class="header__logo">
            <img width="176" src="../../../_static/img/logo-alation.svg" alt="Alation" />
        </a>
        <ul class="header__links">
            <li><a href="https://help.alation.com/s/">Alation Help Center</a></li>
        </ul>
    </div>
</header>

  <div class="wy-grid-for-nav wy-max-content-width">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-scroll__content">
          <div class="wy-side-nav-search">
            

            <ul class="sidebartop">
              <li class="nolink search"><div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search this project" aria-label="Search this project" id="searchinput" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
</li>

            </ul>

            
          </div>

          <div class="wy-menu wy-sidebar wy-sidebar--hidden" data-spy="affix" role="navigation" aria-label="main navigation">
            
              
              
              
                  <p class="caption" role="heading"><span class="caption-text">Welcome to Alation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/About/index.html">About Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/CloudAndOnPrem/index.html">Alation Cloud and On-Premise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/UserProfileandPreferences/index.html">User Profile and Preferences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/CatalogBasics/index.html">Catalog Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/CatalogPages/index.html">Catalog Pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/Conversations/index.html">Conversations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/Domains/index.html">Domains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/BestPractices/index.html">Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../welcome/Glossary/index.html">Alation Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation Cloud Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cloud/StatusPage/index.html">Status Page</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cloud/CloudOverview/index.html">Cloud Service Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cloud/CloudConfiguration/index.html">Configuration for Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cloud/AlationAgent/index.html">Alation Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation Integration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../integration/AlationAnywhere/index.html">Alation Anywhere</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../integration/AlationConnectedSheets/index.html">Alation Connected Sheets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation &amp; Configuration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/ServerInstallation/index.html">Install Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/HighAvailability/index.html">High Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/Update/index.html">Update Alation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/BackupandRestore/index.html">Back Up and Restore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/LineageV2/index.html">Lineage V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/LineageV3/index.html">Lineage V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/AlationAnalyticsV2/index.html">Alation Analytics V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installconfig/AlationContainerService/index.html">Alation Container Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sources</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../CatalogSources/index.html">Catalog Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../VirtualDataSources/index.html">Virtual Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Lineage/index.html">Lineage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../APIResources/index.html">API Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../WorkwithCatalogData/index.html">Working with Catalog Data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Open Connector Framework</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../OCFOverview.html">Open Connector Framework (OCF) Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OCFInstallAlationConnectorManager.html">Install Alation Connector Manager</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AutoInstallDockerAsDependency.html">Auto-Install Docker as Alation Connector Manager Dependency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../InstallDocker2020_3_To_2021_1.html">Install Docker for Alation Versions 2020.3.x - 2021.1.x</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OCFInstallBIConnectors.html">Install OCF BI Connectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ManageConnectors.html">Manage Connectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OCFMaintainConnectors.html">Maintaining OCF Connectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OCFConnectorCatalogUserExperience.html">OCF Connector Catalog User Experience</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OCFTroubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MigrateNativeSourcestoOCF/index.html">Migrate Native Sources to Open Connector Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ConfigureSecretsforOCFConnectors/index.html">Configure Secrets for OCF Connector Settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ConfigureAuthWithIAMRole.html">Configure Authentication via AWS STS and an IAM Role</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ConfigureMDEforOCFDataSources.html">Configure Metadata Extraction for OCF Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ConfigureSamplingforOCFDataSources.html">Configure Sampling and Profiling for OCF Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ConfigureComposeforOCFDataSources.html">Configure Compose for OCF Data Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SchemaPathPattern.html">Schema Path Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BIConnectionInfoField.html">BI Connection Info</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RecreateDatabricksInitScriptUnderWorkspace.html">Recreate Databricks Logging Script Under Workspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AthenaOCFConnector/index.html">Amazon Athena OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AWSGlue/index.html">AWS Glue OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AWSS3/index.html">Amazon S3 OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AzureBlobStorage/index.html">Azure Blob Storage OCF Connector</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Azure Databricks OCF Connector</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="AzureDatabricksOCFConnectorOverview.html">Azure Databricks OCF Connector: Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="AzureDatabricksOCFConnectorInstallandConfigure.html">Azure Databricks OCF Connector: Install and Configure</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Azure Databricks OCF Connector: Query Log Ingestion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../AzureSQLDBOCFConnector/index.html">Azure SQL DB OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AzureSynapseOCFConnector/index.html">Azure Synapse Analytics OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DatabricksonAWS/index.html">Databricks on AWS OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DatabricksUnityCatalog/index.html">Databricks Unity Catalog OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DBT/index.html">Add-On OCF Connector for dbt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Denodo/index.html">Denodo OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GBQOCFConnector/index.html">Google BigQuery OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Greenplum/index.html">Greenplum OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../HiveOCFConnector/index.html">Hive OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../IBMDb2/index.html">IBM Db2 Database OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ImpalaOnCDH_OCFConnector/index.html">Impala on CDH OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ImpalaOnCDP_OCFConnector/index.html">Impala on CDP OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../InformaticaPowerCenter/index.html">Informatica PowerCenter OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MySQL/index.html">MySQL OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Looker/index.html">Looker OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../OracleOCFConnector/index.html">Oracle OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../PostgreSQLOCFConnector/index.html">PostgreSQL OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../PowerBIOCFConnector/index.html">Azure Power BI OCF Connector (Legacy)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../PowerBIScannerOCFConnector/index.html">Azure Power BI Scanner OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QlikSense/index.html">QlikSense OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Redshift/index.html">Amazon Redshift OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPASE/index.html">SAP ASE (Sybase ASE) OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPBOConnector/index.html">SAP BusinessObjects OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPBWConnector/index.html">SAP BW OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPECC/index.html">SAP ECC OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPHANA/index.html">SAP HANA OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SAPIQ/index.html">SAP IQ OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SingleStore/index.html">SingleStore OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Snowflake/index.html">Snowflake OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SQLServer/index.html">SQL Server OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SSRS/index.html">SSRS OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StarburstOCFConnector/index.html">Starburst Enterprise (Trino) OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../TableauOCFConnector/index.html">Tableau OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Teradata/index.html">Teradata OCF Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Vertica/index.html">Vertica OCF Connector</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/DSConfiguration/index.html">Data Source Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/AddDataSources/index.html">Adding Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/ComposeSSOAWSDataSources/index.html">Compose SSO for Amazon Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/ComposeSSOAzureDataSources/index.html">Compose SSO for Azure Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/AuthenticationForExtraction/index.html">Authentication for Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/CustomDB/index.html">Custom DB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/GBQ/index.html">Google BigQuery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasources/Hive/index.html">Hive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">BI Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../bisources/AddBISources/index.html">Add BI Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bisources/AddTableauServer/index.html">Add Tableau Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bisources/WorkwithBISources/index.html">Working with BI Sources</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">File System Sources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../filesystems/FileSystems/index.html">Add File Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../filesystems/VirtualFileSystems/index.html">Virtual File Systems</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Analysts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analyst/UserAuthenticationForDataSources/index.html">User Authentication for Data Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analyst/ShareAndAccessQueries/index.html">Share and Access Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analyst/WriteQueries/index.html">Write Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analyst/DevelopQueries/index.html">Develop Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analyst/WorkwithQueryResults/index.html">Work with Query Results</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Data Stewards</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/GovernanceApp/index.html">Governance App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/PolicyCenter/index.html">Policy Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/Workflow/index.html">Workflow Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/GovernanceDashboard/index.html">Governance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/StewardshipWorkbench/index.html">Stewardship Workbench</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/AnalyticsStewardship/index.html">Analytics Stewardship</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/Glossary/index.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/ArticlesAndTags/index.html">Articles and Tags</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/ArticleGroups/index.html">Article Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/DataCatalogCustomization/index.html">Customize Your Data Catalog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/DataDocumentation/index.html">Data Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/AutoTitlingandLexicon/index.html">Auto-titling and Lexicon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../steward/SnowflakeTags/index.html">Snowflake Tags</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Alation for Server Administrators</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/CustomizableHomePage/index.html">Customizable Homepage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/AdminSettings/index.html">Administrator Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/AdditionalConfiguration/index.html">Additional Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/RunbookForAdministrators/index.html">Runbook for Administrators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/AlationAPIs/index.html">Alation APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/How-tos/index.html">How-tos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../admins/Troubleshooting/index.html">Troubleshooting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Release Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/releasenotes/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/supportmatrices/index.html">Support Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/endofsupport/index.html">End Of Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/snowflakeclladdon/index.html">Snowflake Parser Add-On</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/fieldnotices/index.html">Important Notices and Security Alerts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Archived Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../archive/About/index.html">About the Documentation Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../archive/AdminConfig/index.html">Administration and Update Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../archive/ReleaseNotes/index.html">Release Notes Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../archive/Usage/index.html">Usage and Customization Archive</a></li>
</ul>

              
            
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="wy-nav-top-icon"></i>
        <a href="../../../index.html">Alation User Guide</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">

          <div role="navigation" aria-label="breadcrumbs" class="breadcrumbs">
    <a href="../../../index.html">Docs</a>
    <span class="separator">&gt;</span>
    <a href="../index.html">Open Connector Framework</a>
    <span class="separator">&gt;</span>
    <a href="index.html">Azure Databricks OCF Connector</a>
    <span class="separator">&gt;</span>
    <span>Azure Databricks OCF Connector: Query Log Ingestion</span>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody" class="toBeIndexed">
            
  <section id="azure-databricks-ocf-connector-query-log-ingestion">
<h1>Azure Databricks OCF Connector: Query Log Ingestion<a class="headerlink" href="#azure-databricks-ocf-connector-query-log-ingestion" title="Permalink to this headline">¶</a></h1>
<p class="cloud-label hover-text">Alation Cloud Service <span class="tooltip-text">Applies to Alation Cloud Service instances of Alation</span></p>
<p class="on-prem-label hover-text">Customer Managed <span class="tooltip-text">Applies to customer-managed instances of Alation</span></p>
<p>Query log ingestion (QLI) is supported for both standard and high concurrency Databricks clusters.</p>
<p>Alation offers two approaches to QLI configuration for Azure Databricks:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>File-based QLI</strong> (recommended)—You will need to place the Databricks logs in a container under an Azure Storage account. Alation connects to the storage account to retrieve, parse, and ingest the query history information from the logs.</p></li>
<li><p><strong>Table-based QLI</strong>—You will need to off-load the Databricks logs into a dedicated directory on the Databricks File System (DBFS) or Azure Storage and create an external table and a view on top of the table. Alation will query the view to retrieve query history into the catalog.</p></li>
</ul>
</div></blockquote>
<p>QLI configuration includes steps in Azure Databricks and Alation. Follow this path:</p>
<blockquote>
<div><ul class="simple">
<li><p>Review and fulfill the <a class="reference internal" href="#prerequisite">Prerequisite</a>.</p></li>
<li><p><a class="reference internal" href="#enable-logging-in-databricks">Enable Logging in Databricks</a>.</p></li>
<li><p><a class="reference internal" href="#id1">Configure File-Based QLI</a> or <a class="reference internal" href="#id2">Configure Table-Based QLI</a>.</p></li>
<li><p><a class="reference internal" href="#run-qli">Run QLI</a>.</p></li>
</ul>
</div></blockquote>
<section id="prerequisite">
<h2>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline">¶</a></h2>
<p>The prerequisite is mandatory for file-based QLI and optional for table-based QLI.</p>
<p>For <strong>file-based QLI</strong>, you must provide storage on Azure Storage for raw Databricks logs to be written to. You will need to mount the storage container onto DBFS. This makes the logs accessible for Alation to extract. See <a class="reference internal" href="#supported-azure-storage-types">Supported Azure Storage Types</a>.</p>
<p>For <strong>table-based QLI</strong>, you can either use DBFS to store the logs or choose to store them externally on Azure Storage by creating a mount. Even if you choose to create a mount, you won’t need to provide access to the Azure Storage account to Alation. In case of table-based QLI, Alation does not directly access log files. It will query a dedicated view that you create as part of the configuration.</p>
<section id="supported-azure-storage-types">
<h3>Supported Azure Storage Types<a class="headerlink" href="#supported-azure-storage-types" title="Permalink to this headline">¶</a></h3>
<p>Alation supports these storage types:</p>
<blockquote>
<div><ul class="simple">
<li><p>Azure Blob Storage</p></li>
<li><p>Azure Data Lake Storage Gen 2</p></li>
<li><p>Azure Data Lake Storage Gen 1</p></li>
</ul>
</div></blockquote>
<p>We recommend creating a separate dedicated container under a storage account that you have access to for storing the Databricks logs. Collect the connection information for your storage account before configuring QLI in Alation. You will need different information, depending on the storage account type.</p>
<section id="azure-blob-storage-or-azure-data-lake-storage-gen-2">
<h4>Azure Blob Storage or Azure Data Lake Storage Gen 2<a class="headerlink" href="#azure-blob-storage-or-azure-data-lake-storage-gen-2" title="Permalink to this headline">¶</a></h4>
<p>For Blob Storage or Azure Data Lake Storage Gen 2 account types, Alation supports authentication with <a class="reference external" href="https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal">access keys</a> or <a class="reference external" href="https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview">shared access signatures</a>. Authentication via a service principal is currently not supported.</p>
<p>Prepare the following authentication information for configuration of QLI in Alation:</p>
<blockquote>
<div><ul class="simple">
<li><p>Storage account name</p></li>
<li><p>Access key or shared access signature</p></li>
<li><p>Container name</p></li>
<li><p>Storage endpoint suffix</p></li>
</ul>
</div></blockquote>
</section>
<section id="azure-data-lake-storage-gen-1">
<h4>Azure Data Lake Storage Gen 1<a class="headerlink" href="#azure-data-lake-storage-gen-1" title="Permalink to this headline">¶</a></h4>
<p>For Azure Data Lake Storage Gen1 storage accounts, authentication goes via a <a class="reference external" href="https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals?tabs=browser">service principal</a>.</p>
<p>Prepare the following authentication information for configuration of QLI in Alation:</p>
<blockquote>
<div><ul class="simple">
<li><p>Azure Data Lake Storage URI</p></li>
<li><p>Tenant ID</p></li>
<li><p>Client ID</p></li>
<li><p>Client secret</p></li>
<li><p>Storage endpoint suffix</p></li>
</ul>
</div></blockquote>
</section>
</section>
</section>
<section id="enable-logging-in-databricks">
<h2>Enable Logging in Databricks<a class="headerlink" href="#enable-logging-in-databricks" title="Permalink to this headline">¶</a></h2>
<p>QLI configuration requires logging to be enabled on your Databricks cluster.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Databricks has deprecated init scripts stored on DBFS. Alation previously recommended storing the init script for enabling the logs on DBFS:</p>
<blockquote>
<div><ul class="simple">
<li><p>If you previously configured QLI, you must recreate the init script that enables logging to be stored as a workspace file. Refer to <a class="reference internal" href="../RecreateDatabricksInitScriptUnderWorkspace.html"><span class="doc">Recreate Databricks Logging Script Under Workspace</span></a>.</p></li>
<li><p>If you are initially setting up QLI for an Azure Databricks data source, follow the steps below.</p></li>
</ul>
</div></blockquote>
</div>
<p>Perform these steps to enable logging in Databricks. As a result of this configuration, you will set up the log path directory, enable the DEBUG-level logs, and set a pattern in which logs will be written.</p>
<p>To set up the logs directory:</p>
<ol class="arabic">
<li><p>If you are configuring <strong>file-based QLI</strong>, you must mount external storage on Azure Storage to DBFS, as Alation cannot reach DBFS directly to read the log files.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are configuring table-based QLI, mounting external storage is optional. You can use an internal cluster log path. If you do not wish to create any mounts, skip this step and go to step 2.</p>
</div>
<p>We’ll use an example for a Blob Storage account accessible with the access key. Use the template below to write the mounting script and test it. Replace the placeholder values with real values:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;container-name&gt;</span></code>—Name of the Blob container that you created under a Blob Storage account to store the logs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;storage-account-name&gt;</span></code>—Name of your Azure Storage account.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;mount-name&gt;</span></code>—Name of the mount. You can use the container name to name your mount or use another name.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;access-key&gt;</span></code>—Access key for the storage account.</p></li>
</ul>
</div></blockquote>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Databricks notebook source</span>
<span class="c1"># COMMAND ----------</span>
<span class="c1"># Set up Mount</span>
dbutils.fs.mount<span class="o">(</span>
<span class="w">  </span><span class="nb">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;wasbs://&lt;container-name&gt;@&lt;storage-account-name&gt;.blob.core.windows.net&quot;</span>,
<span class="w">  </span><span class="nv">mount_point</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;/mnt/&lt;mount-name&gt;&quot;</span>,
<span class="w">  </span><span class="nv">extra_configs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="s2">&quot;fs.azure.account.key.&lt;storage-account-name&gt;.blob.core.windows.net&quot;</span>:<span class="s2">&quot;&lt;access-key&gt;&quot;</span><span class="o">})</span>

<span class="c1"># COMMAND ----------</span>
<span class="c1"># Test Mount</span>
dbutils.fs.ls<span class="o">(</span><span class="s2">&quot;/mnt/&lt;mount-name&gt;&quot;</span><span class="o">)</span>

<span class="c1"># COMMAND ----------</span>
<span class="c1"># Test write to Mount - Check in Azure Portal</span>
dbutils.fs.put<span class="o">(</span><span class="s2">&quot;/mnt/&lt;mount-name&gt;/test.txt&quot;</span>,<span class="w"> </span><span class="s2">&quot;this is a test&quot;</span><span class="o">)</span>

<span class="c1"># COMMAND ----------</span>
<span class="c1"># Clean up test</span>
dbutils.fs.rm<span class="o">(</span><span class="s2">&quot;/mnt/&lt;mount-name&gt;/test.txt&quot;</span><span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>In the Databricks user interface, go to the <strong>Configuration</strong> tab for your cluster, and expand <strong>Advanced Options</strong>.</p></li>
<li><p>Open the <a class="reference external" href="https://learn.microsoft.com/en-us/azure/databricks/archive/compute/configure#cluster-log-delivery">Logging</a> tab.</p></li>
<li><p>Specify <strong>Destination</strong> and <strong>Cluster Log Path</strong>:</p>
<blockquote>
<div><ul>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">DBFS</span></code> as <strong>Destination</strong>.</p></li>
<li><p>Set <strong>Cluster Log Path</strong> to your log storage directory:</p>
<blockquote>
<div><ul>
<li><p>For a mounted directory, use format <code class="docutils literal notranslate"><span class="pre">dbfs:/mnt/&lt;mount-name&gt;</span></code></p>
<p>Example: <code class="docutils literal notranslate"><span class="pre">dbfs:/mnt/azdb-databricks-logs</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You don’t need to specify the cluster ID when editing the path. It will be auto-appended when you save your input.</p>
</div>
</li>
<li><p>For an internal DBFS path, use format <code class="docutils literal notranslate"><span class="pre">dbfs:/&lt;directory-name&gt;</span></code></p>
<p>Example: <code class="docutils literal notranslate"><span class="pre">dbfs:/cluster-logs</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can only use an internal DBFS path if you are going to configure table-based QLI. You cannot use it for file-based QLI.</p>
</div>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p>Click <strong>Confirm</strong> to save the value.</p></li>
<li><p>Next, you will enable logging. Alation recommends using an init script that will automatically run when the cluster is restarted. You can add the script via the Databricks user interface.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you cannot use an init script, you can alternatively enable logging using a non-init Python or Scala script and run it every time you restart the cluster. See <a class="reference internal" href="#enable-logging-using-a-regular-script">Enable Logging Using a Regular Script</a> below.</p>
</div>
</div></blockquote>
</li>
</ol>
<p>To enable logging:</p>
<ol class="arabic">
<li><p>Go to your workspace and create a new folder, for example, <code class="docutils literal notranslate"><span class="pre">alation_init_scripts</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can give the new folder any name. We’re using <code class="docutils literal notranslate"><span class="pre">alation_init_scripts</span></code> as an example.</p>
</div>
</div></blockquote>
</li>
<li><p>Set the permissions on the folder to be <strong>Can Manage</strong> for <strong>Admins</strong>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend restricting the permissions to Admins only to ensure that unauthorized users cannot manage this folder and modify the init script.</p>
</div>
</div></blockquote>
</li>
<li><p>Create a new file, for example <code class="docutils literal notranslate"><span class="pre">alation_qli_init_sript.sh</span></code>. Ensure you use the <strong>.sh</strong> extension.</p></li>
<li><p>Copy the script given below in the file you created and review the contents to ensure there are no extra spaces or line breaks that could have been accidentally added while copy-pasting.</p></li>
</ol>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Databricks version 11.x and newer</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Databricks versions older than 11.x</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Executing Init script for Alation QLI&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Executing on Driver:&quot;</span>
<span class="nv">LOG4J_PATH</span><span class="o">=</span><span class="s2">&quot;/home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j2.xml&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Adjusting log4j2.xml here: </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span><span class="s2">&quot;</span>
sed<span class="w"> </span>-i<span class="w"> </span><span class="s1">&#39;0,/&lt;PatternLayout pattern=&quot;%d{yy\/MM\/dd HH:mm:ss} %p %c{1}: %m%n%ex&quot;\/&gt;/s//&lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss.SS} [%t] %p %c{1}: %m%n&quot;\/&gt;/&#39;</span><span class="w"> </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span>
sed<span class="w"> </span>-i<span class="w"> </span><span class="s1">&#39;s/&lt;\/Loggers&gt;/&lt;Logger name=&quot;org.apache.spark.sql.execution.SparkSqlParser&quot; level=&quot;DEBUG&quot;\/&gt;&lt;\/Loggers&gt;/&#39;</span><span class="w"> </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Executing on Driver: </span><span class="nv">$DB_IS_DRIVER</span><span class="s2">&quot;</span>
<span class="k">if</span><span class="w"> </span><span class="o">[[</span><span class="w"> </span><span class="nv">$DB_IS_DRIVER</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;TRUE&quot;</span><span class="w"> </span><span class="o">]]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="nv">LOG4J_PATH</span><span class="o">=</span><span class="s2">&quot;/home/ubuntu/databricks/spark/dbconf/log4j/driver/log4j.properties&quot;</span>
<span class="k">else</span>
<span class="nv">LOG4J_PATH</span><span class="o">=</span><span class="s2">&quot;/home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties&quot;</span>
<span class="k">fi</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Adjusting log4j.properties here: </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;log4j.logger.org.apache.spark.sql.execution.SparkSqlParser=DEBUG&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;log4j.appender.publicFile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss.SS} [%t] %p %c{1}: %m%n&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span><span class="si">${</span><span class="nv">LOG4J_PATH</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;, True)</span>
</pre></div>
</div>
</div></div>
<ol class="arabic simple" start="5">
<li><p>Configure the cluster to run the init script. Follow these steps in Databricks documentation: <a class="reference external" href="https://learn.microsoft.com/en-us/azure/databricks/init-scripts/cluster-scoped#configure-a-cluster-scoped-init-script-using-the-ui">Configure a cluster-scoped init script using the UI</a>.</p></li>
<li><p>Restart the cluster.</p></li>
<li><p>Alation does not process active logs. It only ingests archived log files in the <strong>.log.gz</strong> format. There is a delay between writing contents to a log file and converting the log file into a compressed <strong>.log.gz</strong> file. Before you run QLI in Alation, ensure that your log storage container or the DBFS directory has archived logs.</p></li>
</ol>
<p>Next, configure file-based QLI or table-based QLI:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#configure-file-based-qli"><span class="std std-ref">Configure File-Based QLI</span></a></p></li>
<li><p><a class="reference internal" href="#configure-table-based-qli"><span class="std std-ref">Configure Table-Based QLI</span></a></p></li>
</ul>
</div></blockquote>
<section id="enable-logging-using-a-regular-script">
<h3>Enable Logging Using a Regular Script<a class="headerlink" href="#enable-logging-using-a-regular-script" title="Permalink to this headline">¶</a></h3>
<p>If for some reason you cannot use init scripts on your cluster, you can use a regular script to enable logging.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">Databricks version 11.x and newer</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Databricks versions older than 11.x</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><ol class="arabic">
<li><p>Open a <a class="reference external" href="https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage">Databricks Scala notebook</a>.</p></li>
<li><p>Copy the contents of the Scala script below to enable DEBUG query logs that contain SQL queries and set the pattern in which logs have to show up. This Scala script should be run whenever the cluster is started or restarted.</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>%scala
import<span class="w"> </span>org.apache.logging.log4j.LogManager
import<span class="w"> </span>org.apache.logging.log4j.core.LoggerContext
import<span class="w"> </span>org.apache.logging.log4j.core.config.<span class="o">{</span>Configuration,<span class="w"> </span>LoggerConfig<span class="o">}</span>
import<span class="w"> </span>org.apache.logging.log4j.core.config.Configurator
import<span class="w"> </span>org.apache.logging.log4j.Level
import<span class="w"> </span>org.apache.logging.log4j.core.layout.PatternLayout
import<span class="w"> </span>org.apache.logging.log4j.core.appender.RollingFileAppender<span class="p">;</span>
import<span class="w"> </span>org.apache.logging.log4j.core.appender.RollingFileAppender.Builder<span class="p">;</span>
import<span class="w"> </span>org.apache.logging.log4j.core.filter.AbstractFilterable<span class="p">;</span>
import<span class="w"> </span>org.apache.logging.log4j.core.config.AppenderRef<span class="p">;</span>
import<span class="w"> </span>org.apache.logging.log4j.core.appender.rewrite.RewriteAppender<span class="p">;</span>
import<span class="w"> </span>com.databricks.logging.ServiceRewriteAppender<span class="p">;</span>
import<span class="w"> </span>org.apache.logging.log4j.core.config.AbstractConfiguration<span class="p">;</span>

Configurator.setRootLevel<span class="o">(</span>Level.DEBUG<span class="o">)</span><span class="p">;</span>

val<span class="w"> </span><span class="nv">ctx</span><span class="w"> </span><span class="o">=</span><span class="w">  </span>LogManager.getContext<span class="o">(</span><span class="nb">false</span><span class="o">)</span>.asInstanceOf<span class="o">[</span>LoggerContext<span class="o">]</span><span class="p">;</span>
val<span class="w"> </span><span class="nv">conf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>ctx.getConfiguration<span class="o">()</span><span class="p">;</span>
val<span class="w"> </span><span class="nv">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>PatternLayout.newBuilder<span class="o">()</span>
<span class="w">      </span>.withConfiguration<span class="o">(</span>conf<span class="o">)</span>
<span class="w">      </span>.withPattern<span class="o">(</span><span class="s2">&quot;%d{yyyy-MM-dd HH:mm:ss.SS} [%t] %p %c{1}: %m%n&quot;</span><span class="o">)</span>
<span class="w">      </span>.build<span class="o">()</span><span class="p">;</span>

val<span class="w"> </span><span class="nv">rollingFileAppender</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>conf.getAppenders<span class="o">()</span>.get<span class="o">(</span><span class="s2">&quot;publicFile.rolling&quot;</span><span class="o">)</span>.asInstanceOf<span class="o">[</span>RollingFileAppender<span class="o">]</span><span class="p">;</span>
val<span class="w"> </span>appenderBuilder:<span class="w"> </span>RollingFileAppender.Builder<span class="o">[</span>_<span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>RollingFileAppender.newBuilder<span class="o">()</span><span class="p">;</span>
appenderBuilder.setConfiguration<span class="o">(</span>conf<span class="o">)</span>
appenderBuilder.setName<span class="o">(</span>rollingFileAppender.getName<span class="o">())</span>
appenderBuilder.setLayout<span class="o">(</span>layout<span class="o">)</span>
appenderBuilder.withFileName<span class="o">(</span>rollingFileAppender.getFileName<span class="o">())</span>
appenderBuilder.withFilePattern<span class="o">(</span>rollingFileAppender.getFilePattern<span class="o">())</span>
appenderBuilder.withPolicy<span class="o">(</span>rollingFileAppender.getTriggeringPolicy<span class="o">())</span>
appenderBuilder.setBufferedIo<span class="o">(</span><span class="nb">false</span><span class="o">)</span>
appenderBuilder.setBufferSize<span class="o">(</span>rollingFileAppender.getManager<span class="o">()</span>.getBufferSize<span class="o">())</span>
appenderBuilder.setImmediateFlush<span class="o">(</span>rollingFileAppender.getImmediateFlush<span class="o">())</span>
appenderBuilder.withCreateOnDemand<span class="o">(</span>rollingFileAppender.getManager<span class="o">()</span>.isCreateOnDemand<span class="o">())</span>
val<span class="w"> </span><span class="nv">appender</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>appenderBuilder.build<span class="o">()</span><span class="p">;</span>

val<span class="w"> </span><span class="nv">appenderRef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>Array<span class="o">(</span>AppenderRef.createAppenderRef<span class="o">(</span>appender.getName<span class="o">()</span>,<span class="w"> </span>null,<span class="w"> </span>null<span class="o">))</span><span class="p">;</span>
var<span class="w"> </span><span class="nv">policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>new<span class="w"> </span>ServiceRewriteAppender<span class="o">()</span><span class="p">;</span>

val<span class="w"> </span><span class="nv">rewriteAppender</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>conf.getAppenders<span class="o">()</span>.get<span class="o">(</span><span class="s2">&quot;publicFile.rolling.rewrite&quot;</span><span class="o">)</span>.asInstanceOf<span class="o">[</span>RewriteAppender<span class="o">]</span><span class="p">;</span>
val<span class="w"> </span><span class="nv">updatedRewriteAppender</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>RewriteAppender.createAppender<span class="o">(</span>rewriteAppender.getName<span class="o">()</span>,<span class="w"> </span>String.valueOf<span class="o">(</span>rewriteAppender.ignoreExceptions<span class="o">())</span>,<span class="w"> </span>appenderRef,<span class="w"> </span>conf,<span class="w"> </span>policy,<span class="w"> </span>rewriteAppender.getFilter<span class="o">())</span><span class="p">;</span>

rollingFileAppender.stop<span class="o">()</span><span class="p">;</span>
rewriteAppender.stop<span class="o">()</span><span class="p">;</span>

val<span class="w"> </span><span class="nv">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>ctx.getConfiguration<span class="o">()</span>.asInstanceOf<span class="o">[</span>AbstractConfiguration<span class="o">]</span><span class="p">;</span>
config.removeAppender<span class="o">(</span>rollingFileAppender.getName<span class="o">())</span><span class="p">;</span>
config.removeAppender<span class="o">(</span>rewriteAppender.getName<span class="o">())</span><span class="p">;</span>

conf.addAppender<span class="o">(</span>appender<span class="o">)</span><span class="p">;</span>
conf.addAppender<span class="o">(</span>updatedRewriteAppender<span class="o">)</span><span class="p">;</span>
appender.start<span class="o">()</span><span class="p">;</span>
updatedRewriteAppender.start<span class="o">()</span><span class="p">;</span>

conf.getRootLogger<span class="o">()</span>.addAppender<span class="o">(</span>updatedRewriteAppender,<span class="w"> </span>null,<span class="w"> </span>null<span class="o">)</span><span class="p">;</span>
ctx.updateLoggers<span class="o">()</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><ol class="arabic">
<li><p>Open a <a class="reference external" href="https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage">Databricks Python notebook</a>.</p></li>
<li><p>Copy the contents of the Python script below to enable DEBUG query logs that contain SQL queries and set the pattern in which logs have to show up. This Python script should be run whenever the cluster is started or restarted.</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="nv">log4j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>spark.sparkContext._jvm.org.apache.log4j
log4j.LogManager.getRootLogger<span class="o">()</span>.setLevel<span class="o">(</span>log4j.Level.DEBUG<span class="o">)</span>
<span class="nv">ca</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>log4j.LogManager.getRootLogger<span class="o">()</span>.getAppender<span class="o">(</span><span class="s2">&quot;publicFile&quot;</span><span class="o">)</span>
ca.setLayout<span class="o">(</span>log4j.PatternLayout<span class="o">(</span><span class="s2">&quot;%d{yyyy-MM-dd HH:mm:ss.SS} [%t] %p %c{1}: %m%n&quot;</span><span class="o">))</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</div></div>
<p>Next, configure file-based QLI or table-based QLI:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#configure-file-based-qli"><span class="std std-ref">Configure File-Based QLI</span></a></p></li>
<li><p><a class="reference internal" href="#configure-table-based-qli"><span class="std std-ref">Configure Table-Based QLI</span></a></p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="configure-file-based-qli">
<span id="id1"></span><h2>Configure File-Based QLI<a class="headerlink" href="#configure-file-based-qli" title="Permalink to this headline">¶</a></h2>
<p>File-based QLI is a configuration where Alation will connect to Azure Storage and read and process raw driver logs stored in a container. The logs are parsed based on a log extraction template in the JSON format. You create the JSON content as part of the configuration in Alation (described below).</p>
<p><em>File-based QLI flowchart</em></p>
<a class="with-border reference internal image-reference" href="../../../_images/OCF_Azure_Databricks_FileBasedDiagram.png"><img alt="../../../_images/OCF_Azure_Databricks_FileBasedDiagram.png" class="with-border" src="../../../_images/OCF_Azure_Databricks_FileBasedDiagram.png" style="width: 600px;" /></a>
<p>To set up file-based QLI:</p>
<ol class="arabic">
<li><p>In Alation, open the Settings page of your Azure Databricks OCF data source.</p></li>
<li><p>Open the <strong>Query Log Ingestion</strong> tab.</p></li>
<li><p>Under <strong>Connector Settings</strong> and <strong>Query Log Ingestion Type</strong>, select the <strong>File based query log ingestion</strong> radio button.</p>
<blockquote>
<div><a class="with-border reference internal image-reference" href="../../../_images/OCF_Azure_Databricks_SelectFileBasedQLI.png"><img alt="../../../_images/OCF_Azure_Databricks_SelectFileBasedQLI.png" class="with-border" src="../../../_images/OCF_Azure_Databricks_SelectFileBasedQLI.png" style="width: 300px;" /></a>
</div></blockquote>
</li>
<li><p>Click <strong>Save</strong> in this section.</p></li>
<li><p>Below the <strong>Connector Settings</strong> section, locate section <strong>File Based Log Query Log Ingestion</strong>. Select the appropriate storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Azure Blob Storage</strong>—Select this radio button if your log storage container is under a Blob Storage or an Azure Data Lake Storage Gen 2 account.</p></li>
<li><p><strong>ADLS</strong>—Select this radio button if your log storage container is under an Azure Data Lake Storage Gen 1 account.</p></li>
</ul>
<a class="with-border reference internal image-reference" href="../../../_images/OCF_Azure_Databricks_SelectStorage.png"><img alt="../../../_images/OCF_Azure_Databricks_SelectStorage.png" class="with-border" src="../../../_images/OCF_Azure_Databricks_SelectStorage.png" style="width: 300px;" /></a>
</div></blockquote>
</li>
<li><p>Next, you will need to fill in the Log Extraction Configuration JSON field. Use the template given below.</p>
<blockquote>
<div><p>In your JSON content, change the value of the <code class="docutils literal notranslate"><span class="pre">&lt;your-path&gt;</span></code> key to the path to the folder that stores the logs in the Azure Storage container, for example:</p>
<p><code class="docutils literal notranslate"><span class="pre">/cluster-logs/0130-102557-aft119/driver/</span></code></p>
<p>Where:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/cluster-logs</span></code> is the subfolder in the Azure Storage container.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">0130-102557-aft119</span></code> is your cluster ID.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/driver/</span></code> is the auto-created directory that stores the log files that Alation will process.</p></li>
</ul>
</div></blockquote>
<p><em>Log Extraction Configuration JSON</em></p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="w">  </span><span class="s2">&quot;folderPath&quot;</span>:<span class="s2">&quot;&lt;your-path&gt;&quot;</span>,
<span class="w">  </span><span class="s2">&quot;nThread&quot;</span>:<span class="s2">&quot;10&quot;</span>,
<span class="w">  </span><span class="s2">&quot;threadTimeOut&quot;</span>:<span class="s2">&quot;2000&quot;</span>,
<span class="w">  </span><span class="s2">&quot;parserType&quot;</span>:<span class="s2">&quot;LOG4J&quot;</span>,
<span class="w">  </span><span class="s2">&quot;log4jConversionPattern&quot;</span>:<span class="s2">&quot;TIMESTAMP [THREAD] LEVEL LOGGER MESSAGE&quot;</span>,
<span class="w">  </span><span class="s2">&quot;log4jTimeFormat&quot;</span>:<span class="s2">&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;</span>,
<span class="w">  </span><span class="s2">&quot;requiredExtraction&quot;</span>:<span class="o">[</span>
<span class="w">      </span><span class="o">{</span>
<span class="w">        </span><span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractSqlQuery&quot;</span>,
<span class="w">        </span><span class="s2">&quot;keyValuePair&quot;</span>:
<span class="w">        </span><span class="o">{</span>
<span class="w">          </span><span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;SparkSqlParser&quot;</span>,
<span class="w">          </span><span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;Parsing command:(?&lt;queryString&gt;[\\w\\W]*)&quot;</span>
<span class="w">        </span><span class="o">}</span>
<span class="w">      </span><span class="o">}</span>,
<span class="w">      </span><span class="o">{</span>
<span class="w">        </span><span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractUserInfo&quot;</span>,
<span class="w">        </span><span class="s2">&quot;keyValuePair&quot;</span>:
<span class="w">        </span><span class="o">{</span>
<span class="w">          </span><span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;audit&quot;</span>,
<span class="w">          </span><span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;ugi=(?:\\(Basic token\\))?(?&lt;userName&gt;[\\S]+)&quot;</span>
<span class="w">        </span><span class="o">}</span>
<span class="w">      </span><span class="o">}</span>,
<span class="w">      </span><span class="o">{</span>
<span class="w">        </span><span class="s2">&quot;fieldName&quot;</span>:<span class="s2">&quot;extractTimeTaken&quot;</span>,
<span class="w">        </span><span class="s2">&quot;keyValuePair&quot;</span>:
<span class="w">        </span><span class="o">{</span>
<span class="w">          </span><span class="s2">&quot;loggerName&quot;</span>:<span class="s2">&quot;Retrieve&quot;</span>,
<span class="w">          </span><span class="s2">&quot;regex&quot;</span>:<span class="s2">&quot;Execution Time = (?&lt;milliSeconds&gt;[\\d]+)&quot;</span>
<span class="w">        </span><span class="o">}</span>
<span class="w">      </span><span class="o">}</span>
<span class="w">  </span><span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information about this JSON template and the available customization options, refer to the topic that explains the concept of <a class="reference internal" href="../../../datasources/CustomDB/FileBasedQLICustomDB.html"><span class="doc">File-Based Query Log Ingestion</span></a>.</p>
</div>
</div></blockquote>
</li>
<li><p>Click <strong>Save</strong> in this section.</p></li>
<li><p>Depending on the type of your storage account, configure access to logs in the respective section of the <strong>General Settings</strong> tab:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#configure-azure-blob-storage-connection">Configure Azure Blob Storage Connection</a>—Specify information in the <strong>Configure Azure Blob Storage Connection</strong> section if you are storing the logs under a Blob Storage or an Azure Data Lake Storage Gen 2 account.</p></li>
<li><p><a class="reference internal" href="#configure-adls-connection">Configure ADLS Connection</a>—Specify information in the <strong>Configure ADLS Connection</strong> section if you are storing the logs under an Azure Data Lake Storage Gen 1 account.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><a class="reference internal" href="#run-qli">Run QLI</a>.</p></li>
</ol>
<section id="configure-azure-blob-storage-connection">
<h3>Configure Azure Blob Storage Connection<a class="headerlink" href="#configure-azure-blob-storage-connection" title="Permalink to this headline">¶</a></h3>
<p>The section <strong>Configure Azure Blob Storage Connection</strong> applies to both Blob Storage and Azure Data Lake Storage Gen 2 accounts. The user interface currently does not differentiate between these storage account types.</p>
<blockquote>
<div><a class="with-border reference internal image-reference" href="../../../_images/OCF_Azure_Databricks_ConfigureBlob.png"><img alt="../../../_images/OCF_Azure_Databricks_ConfigureBlob.png" class="with-border" src="../../../_images/OCF_Azure_Databricks_ConfigureBlob.png" style="width: 300px;" /></a>
</div></blockquote>
<p>Specify the Blob Storage or Azure Data Lake Storage Gen 2 account authentication details and save the information by clicking <strong>Save</strong>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 31%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Parameter</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Use Shared Access
Signature</p></td>
<td><p>Select this checkbox to use the shared access signature for
authentication. Leave it clear if using an access key.</p></td>
</tr>
<tr class="row-odd"><td><p>Storage Account Name</p></td>
<td><p>Specify the storage account name.</p></td>
</tr>
<tr class="row-even"><td><p>Access Key/Shared Access
Signature</p></td>
<td><p>Paste the shared access signature if the <strong>Use</strong>
<strong>Shared Access Signature</strong> checkbox is selected.
If using an access key, paste the access key.</p></td>
</tr>
<tr class="row-odd"><td><p>Blob Container</p></td>
<td><p>Specify the Blob container name.</p></td>
</tr>
<tr class="row-even"><td><p>Storage Endpoint Suffix</p></td>
<td><p>Specify the storage endpoint suffix
according to the data source:</p>
<blockquote>
<div><ul>
<li><p><strong>Azure Databricks</strong>: <code class="docutils literal notranslate"><span class="pre">core.windows.net</span></code></p>
<p>If you copy the value from endpoint definitions
in Databricks, delete <code class="docutils literal notranslate"><span class="pre">blob.</span></code> and specify
<code class="docutils literal notranslate"><span class="pre">core.windows.net</span></code> only. The value
<code class="docutils literal notranslate"><span class="pre">blob.core.windows.net</span></code> will fail as the connector
code automatically appends <code class="docutils literal notranslate"><span class="pre">blob.</span></code>
to the value you specify.</p>
</li>
<li><p><strong>Azure Databricks on Azure Government Cloud</strong>:
<code class="docutils literal notranslate"><span class="pre">usgovclosapi.net</span></code></p></li>
</ul>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</section>
<section id="configure-adls-connection">
<h3>Configure ADLS Connection<a class="headerlink" href="#configure-adls-connection" title="Permalink to this headline">¶</a></h3>
<p>Under the <strong>Configure ADLS Connection</strong> section, specify the authentication information for an Azure Data Lake Storage Gen 1 account and save the information by clicking <strong>Save</strong>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 31%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Parameter</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ADLS URI</p></td>
<td><p>Specify the ADLS URI.</p></td>
</tr>
<tr class="row-odd"><td><p>Tenant ID</p></td>
<td><p>Specify the tenant ID.</p></td>
</tr>
<tr class="row-even"><td><p>Client ID</p></td>
<td><p>Specify the client ID.</p></td>
</tr>
<tr class="row-odd"><td><p>Client Secret</p></td>
<td><p>Specify the client secret.</p></td>
</tr>
<tr class="row-even"><td><p>Storage Endpoint Suffix</p></td>
<td><p>Specify the storage endpoint suffix with the following suffix
according to the data source:</p>
<blockquote>
<div><ul class="simple">
<li><p>Azure Databricks: <code class="docutils literal notranslate"><span class="pre">windows.net</span></code></p></li>
<li><p>Azure Databricks on Azure Government Cloud:
<code class="docutils literal notranslate"><span class="pre">usgovclosapi.net</span></code>.</p></li>
</ul>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="configure-table-based-qli">
<span id="id2"></span><h2>Configure Table-Based QLI<a class="headerlink" href="#configure-table-based-qli" title="Permalink to this headline">¶</a></h2>
<p>The table-based QLI method uses an external table that is created based on the Databricks logs and a view on top of this table.</p>
<p>You will need to create a log output directory in the cluster log path and run a script that will lift the logs from the logs storage directory, parse them, and store them in the log output directory. After that, you will create an external table from the log output directory and a view on top of the table. Alation will read the view to extract query history.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on external tables in Databricks, refer to the corresponding <a class="reference external" href="https://docs.databricks.com/sql/language-manual/sql-ref-external-tables.html">Databricks documentation</a>.</p>
</div>
</div></blockquote>
<p><em>Table-based QLI flowchart</em></p>
<blockquote>
<div><a class="with-border reference internal image-reference" href="../../../_images/OCF_Azure_Databricks_TableBasedDiagram.png"><img alt="../../../_images/OCF_Azure_Databricks_TableBasedDiagram.png" class="with-border" src="../../../_images/OCF_Azure_Databricks_TableBasedDiagram.png" style="width: 600px;" /></a>
</div></blockquote>
<section id="configuration-in-databricks">
<h3>Configuration in Databricks<a class="headerlink" href="#configuration-in-databricks" title="Permalink to this headline">¶</a></h3>
<p>To configure table-based QLI:</p>
<ol class="arabic">
<li><p>Ensure that you have enabled logging: <a class="reference internal" href="#enable-logging-in-databricks">Enable Logging in Databricks</a>.</p></li>
<li><p>In Databricks, create an additional output directory, <code class="docutils literal notranslate"><span class="pre">logs</span></code>, in the cluster log directory. This directory will be used to write the parsed logs to. Create this directory before running the Python script in step 3. If it is not created, a <em>File Not Found</em> error will occur.</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.mkdirs<span class="o">(</span><span class="s2">&quot;dbfs:/cluster-logs/&lt;cluster-id&gt;/logs/&quot;</span><span class="o">)</span>
</pre></div>
</div>
<p><strong>Example</strong>:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>dbutils.fs.mkdirs<span class="o">(</span><span class="s2">&quot;dbfs:/cluster-logs/0130-102557-aft119/logs/&quot;</span><span class="o">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can create a mount point for this directory if you prefer using external storage.</p>
</div>
</div></blockquote>
</li>
</ol>
<ol class="arabic" start="2">
<li><p>Open a new Python notebook.</p></li>
<li><p>Copy the contents from the Python script below into the notebook cell. The script will consolidate SQL statements which run over several lines into a single line. Replace the values of <code class="docutils literal notranslate"><span class="pre">input_dir</span></code> and <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> with real values:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_dir</span></code>—The directory you find in the <strong>Cluster Log Path</strong> that you specified when enabling the logs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dir</span></code>—The logs output directory you created in step 1.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you are accessing the directories mounted onto the Databricks File System (DBFS), use the prefix <code class="docutils literal notranslate"><span class="pre">/dbfs/mnt/&lt;mounted-filepath&gt;</span></code>, for example: <code class="docutils literal notranslate"><span class="pre">/dbfs/mnt/cluster-logs/0130-102557-aft119/driver/</span></code> and <code class="docutils literal notranslate"><span class="pre">/dbfs/mnt/cluster-logs/0130-102557-aft119/logs/</span></code> where <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs</span></code> is the mount name.</p>
<p>If you are accessing the log files directly on DBFS, use the DBFS filepath <code class="docutils literal notranslate"><span class="pre">/dbfs/&lt;file-path&gt;</span></code>, for example: <code class="docutils literal notranslate"><span class="pre">/dbfs/0130-102557-aft119/driver/</span></code> and <code class="docutils literal notranslate"><span class="pre">/dbfs/cluster-logs/0130-102557-aft119/logs/</span></code>.</p>
<p>If the file path for <code class="docutils literal notranslate"><span class="pre">input_dir</span></code> or <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> is not specified correctly, the script will not be able to locate the files.</p>
</div>
<p><em>Python script to off-load parsed logs</em></p>
<blockquote>
<div><div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="n">input_dir</span> <span class="o">=</span> <span class="s1">&#39;/dbfs/mnt/cluster-logs/0130-102557-aft119/driver/&#39;</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;/dbfs/mnt/cluster-logs/0130-102557-aft119/logs/&#39;</span>

<span class="n">required_logger_line_regex</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;^\d+-\d+-\d+ \d+:\d+:\d+\.\d* \[[^\]]*?\] \S+ (?:SparkSqlParser|audit): .*&#39;</span>
<span class="n">logger_line_regex</span> <span class="o">=</span>  <span class="sa">r</span><span class="s1">&#39;^\d+-\d+-\d+ \d+:\d+:\d+\.\d* \[[^\]]*?\] \S+ \S+:.*&#39;</span>

<span class="n">prev_line</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="n">line_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">file_date_hour</span> <span class="o">=</span> <span class="nb">str</span><span class="p">((</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hours</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">-%H&#39;</span><span class="p">))</span>

<span class="n">input_file_name</span> <span class="o">=</span> <span class="n">input_dir</span> <span class="o">+</span> <span class="s1">&#39;log4j-&#39;</span> <span class="o">+</span> <span class="n">file_date_hour</span> <span class="o">+</span> <span class="s1">&#39;.log.gz&#39;</span>
<span class="n">output_file_name</span> <span class="o">=</span> <span class="n">output_dir</span> <span class="o">+</span> <span class="s1">&#39;log4j-&#39;</span> <span class="o">+</span> <span class="n">file_date_hour</span> <span class="o">+</span> <span class="s1">&#39;.log.gz&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Transforming log4j-</span><span class="si">{}</span><span class="s1">.log.gz&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_date_hour</span><span class="p">))</span>

<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">input_file_name</span><span class="p">,</span> <span class="s1">&#39;rt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">output_file_name</span><span class="p">,</span> <span class="s1">&#39;wt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fin</span><span class="p">:</span>
      <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">required_logger_line_regex</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
          <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
          <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">prev_line</span> <span class="o">=</span> <span class="n">line</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">logger_line_regex</span><span class="p">,</span> <span class="n">line</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">prev_line</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">required_logger_line_regex</span><span class="p">,</span> <span class="n">prev_line</span><span class="p">):</span>
          <span class="n">prev_line</span> <span class="o">=</span> <span class="n">prev_line</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;\n&#39;</span> <span class="o">+</span> <span class="n">line</span>
    <span class="k">if</span> <span class="n">prev_line</span><span class="p">:</span>
      <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prev_line</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="n">line_count</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Lines written: &#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">line_count</span><span class="p">))</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
</li>
<li><p>Run the script after at least one log file has been created and verify that you get output similar to the following:</p>
<blockquote>
<div><ul>
<li><p>Expected output when the file name is <code class="docutils literal notranslate"><span class="pre">log4j-2019-05-02-10.log.gz</span></code> and there is no query or user information in the file:</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>Transforming<span class="w"> </span>log4j-2019-05-02-10.log.gz
Lines<span class="w"> </span>written:<span class="w">  </span><span class="m">0</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Expected output when the file name is <code class="docutils literal notranslate"><span class="pre">log4j-2019-05-02-11.log.gz</span></code> and query or user information is present:</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>Transforming<span class="w"> </span>log4j-2019-05-02-11.log.gz
Lines<span class="w"> </span>written:<span class="w">  </span><span class="m">275</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p>You must schedule this Python script. We recommend scheduling it to run at or after ten minutes past the hour, which ensures the availability of archived logs. If the script is scheduled exactly at the hour, archived log files will not be available, as there is a five-minute delay between writing the contents to the log file and converting the log file into a compressed <strong>.log.gz</strong> file.</p></li>
<li><p>Create an external table with the <code class="docutils literal notranslate"><span class="pre">LOCATION</span></code> property set to the log output directory (<code class="docutils literal notranslate"><span class="pre">output_dir</span></code>):</p>
<blockquote>
<div><ul class="simple">
<li><p>For setting the <code class="docutils literal notranslate"><span class="pre">LOCATION</span></code> property to an external storage directory mounted to DBFS, use the prefix <code class="docutils literal notranslate"><span class="pre">/mnt/&lt;mounted_filepath&gt;</span></code>. Example: <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs/0130-102557-aft119/logs/</span></code>, where <code class="docutils literal notranslate"><span class="pre">/mnt/cluster-logs</span></code> is the mount name.</p></li>
<li><p>For setting the <code class="docutils literal notranslate"><span class="pre">LOCATION</span></code> property to the directory present in DBFS, use the file path. Example: <code class="docutils literal notranslate"><span class="pre">/0206-072111-lox669/logs/</span></code>.</p></li>
</ul>
<p><em>Example query to create the external table</em>:</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">EXTERNAL</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="p">(</span>
<span class="w">  </span><span class="n">date_time_string</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">  </span><span class="n">thread_name</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">  </span><span class="k">level</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">  </span><span class="n">logger</span><span class="w"> </span><span class="n">STRING</span><span class="p">,</span>
<span class="w">  </span><span class="n">message</span><span class="w"> </span><span class="n">STRING</span><span class="p">)</span>
<span class="w">  </span><span class="k">ROW</span><span class="w"> </span><span class="n">FORMAT</span>
<span class="w">  </span><span class="n">SERDE</span><span class="w"> </span><span class="s1">&#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;</span>
<span class="w">  </span><span class="k">WITH</span><span class="w"> </span><span class="n">SERDEPROPERTIES</span>
<span class="w">    </span><span class="p">(</span><span class="ss">&quot;input.regex&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;^(\\S+ \\S+) \\[(.*?)\\] (\\S+) (\\S+): (.*?)&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="k">LOCATION</span>
<span class="w">    </span><span class="ss">&quot;/mnt/cluster-logs/0130-102557-aft119/logs/&quot;</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Ensure that the external table is populated with the data from the files stored in the log output directory (<code class="docutils literal notranslate"><span class="pre">output_dir</span></code>) by running a SELECT query on the table, for example:</p>
<blockquote>
<div><div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="p">;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Create a view that takes this external table as an input and structures the data as required by Alation. See the example SQL below.</p>
<blockquote>
<div><p><em>Example query to create the QLI view</em>:</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">DROP</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span><span class="p">;</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span><span class="w"> </span><span class="k">AS</span>
<span class="k">SELECT</span>
<span class="w">  </span><span class="k">distinct</span><span class="w"> </span><span class="o">*</span><span class="p">,</span>
<span class="w">  </span><span class="n">CONCAT</span><span class="p">(</span><span class="n">userName</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">startTime</span><span class="p">)</span><span class="w"> </span><span class="n">sessionId</span>
<span class="k">FROM</span><span class="w"> </span><span class="p">(</span>
<span class="k">SELECT</span>
<span class="w">  </span><span class="n">a</span><span class="p">.</span><span class="n">date_time_string</span><span class="w"> </span><span class="n">startTime</span><span class="p">,</span>
<span class="w">  </span><span class="n">regexp_extract</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;Parsing command: (.*)&quot;</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="n">queryString</span><span class="p">,</span>
<span class="w">  </span><span class="k">CASE</span>
<span class="w">    </span><span class="k">WHEN</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">null</span><span class="w"> </span><span class="k">THEN</span><span class="w"> </span><span class="s1">&#39;unknown&#39;</span>
<span class="w">    </span><span class="k">WHEN</span><span class="w"> </span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;Basic token&#39;</span>
<span class="w">      </span><span class="k">THEN</span><span class="w"> </span><span class="k">TRIM</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">18</span><span class="p">,</span><span class="w"> </span><span class="n">instr</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ip=&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">19</span><span class="p">))</span>
<span class="w">    </span><span class="k">ELSE</span><span class="w"> </span><span class="k">TRIM</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="n">instr</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">message</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;ip=&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">6</span><span class="p">))</span>
<span class="w">  </span><span class="k">END</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">userName</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="w"> </span><span class="n">a</span>
<span class="k">LEFT</span><span class="w"> </span><span class="k">OUTER</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">external_log_table</span><span class="w"> </span><span class="n">b</span>
<span class="w">  </span><span class="k">ON</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">thread_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">thread_name</span><span class="p">)</span>
<span class="k">WHERE</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">logger</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;SparkSqlParser&#39;</span>
<span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">logger</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;audit&#39;</span>
<span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">date_time_string</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">date_time_string</span><span class="p">);</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Next, perform the configuration in Alation.</p></li>
</ol>
</section>
<section id="configuration-in-alation">
<h3>Configuration in Alation<a class="headerlink" href="#configuration-in-alation" title="Permalink to this headline">¶</a></h3>
<p>To configure table-based QLI:</p>
<ol class="arabic">
<li><p>In Alation, open the Settings page of your Azure Databricks OCF data source.</p></li>
<li><p>Open the <strong>Query Log Ingestion</strong> tab.</p></li>
<li><p>Under <strong>Connector Settings</strong> and <strong>Query Log Ingestion Type</strong>, select the <strong>Table based query log ingestion</strong> radio button.</p></li>
<li><p>In the field <strong>Query to Execute</strong>, enter the query given below.</p>
<blockquote>
<div><ul class="simple">
<li><p>Replace the placeholder value <code class="docutils literal notranslate"><span class="pre">databricks_demo.alation_qli</span></code> with the name of your QLI view in the <code class="docutils literal notranslate"><span class="pre">schema.view</span></code> format.</p></li>
<li><p>Do not substitute values <code class="docutils literal notranslate"><span class="pre">STARTTIME1</span></code> and <code class="docutils literal notranslate"><span class="pre">STARTTIME2</span></code>. Leave them as is.</p></li>
</ul>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span>
<span class="w">  </span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">startTime</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">19</span><span class="p">)</span><span class="w"> </span><span class="n">startTime</span><span class="p">,</span>
<span class="w">  </span><span class="n">queryString</span><span class="p">,</span>
<span class="w">  </span><span class="n">userName</span><span class="p">,</span>
<span class="w">  </span><span class="n">sessionID</span>
<span class="k">FROM</span><span class="w"> </span><span class="n">databricks_demo</span><span class="p">.</span><span class="n">alation_qli</span>
<span class="k">WHERE</span><span class="w"> </span><span class="n">startTime</span><span class="w"> </span><span class="k">BETWEEN</span><span class="w"> </span><span class="s1">&#39;STARTTIME1&#39;</span><span class="w"> </span><span class="k">AND</span><span class="w"> </span><span class="s1">&#39;STARTTIME2&#39;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Click <strong>Save</strong> in the <strong>Query Log Ingestion Type</strong> section. Next, see <a class="reference internal" href="#run-qli">Run QLI</a>.</p></li>
</ol>
</section>
</section>
<section id="run-qli">
<span id="azure-databricks-run-qli"></span><h2>Run QLI<a class="headerlink" href="#run-qli" title="Permalink to this headline">¶</a></h2>
<p>You can either perform QLI manually on demand or enable automated QLI:</p>
<ol class="arabic">
<li><p>To perform manual QLI, under the <strong>Automated and Manual Query Log Ingestion</strong> section of the <strong>Query Log Ingestion</strong> tab, ensure that the <strong>Enable Automated Query Log Ingestion</strong> toggle is disabled.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Metadata extraction must be completed first before running QLI.</p>
</div>
</div></blockquote>
</li>
<li><p>Click <strong>Preview</strong> to get a sample of the query history data to be ingested.</p></li>
<li><p>Click the <strong>Import</strong> button to perform QLI on demand.</p></li>
<li><p>To schedule QLI, enable the <strong>Enable Automated Query Log Ingestion</strong> toggle.</p></li>
<li><p>Set a schedule under <strong>Automated Query Log Ingestion Time</strong> by specifying values in the <em>week</em>, <em>day</em>, and <em>time</em> fields. The next QLI job will run on the schedule you have specified.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The hourly schedule for automated QLI is not supported.</p>
</div>
</div></blockquote>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer>

  
<div class="navigation-bar">

    <div class="navigation navigation--prev">
        
        <a class="navigation__link" href="AzureDatabricksOCFConnectorInstallandConfigure.html">
            <button class="navigation__button">
                <i class="icon icon-left"></i>
            </button>
            <div class="navigation__title">
                <span class="heading">Previous</span> <br />Azure Databricks OCF Connector: Install and Configure
            </div>
        </a>
        
    </div>

    <div class="navigation navigation--next">
        
        <a class="navigation__link" title="Accesskey Alt(+Shift)+p" accesskey="p" href="../AzureSQLDBOCFConnector/index.html">
            <div class="navigation__title">
                <span class="heading">Next</span> <br />Azure SQL DB OCF Connector
            </div>
            <button class="navigation__button" title="Accesskey Alt(+Shift)+n" accesskey="n">
                <i class="icon icon-right"></i>
            </button>
        </a>
        
    </div>

</div>


</footer>

        </div>
      </div>

    </section>

    <div class="wy-nav-side wy-nav-secondary">
      <div class="wy-side-scroll">
        <div class="wy-side-scroll__content">
          <div class="wy-menu-secondary">
              <div class="comments">
    <a href="mailto:no-reply-docs-feedback@alation.com?subject=Documentation Feedback on sources/OpenConnectorFramework/AzureDatabricks/AzureDatabricksOCFConnectorQLI&body=Alation%20welcomes%20your%20comments%20on%20typos,%20content%20accuracy%20and%20completeness,%20and%20whether%20you%20found%20this%20page%20helpful.%20You%20will%20not%20receive%20a%20reply%20to%20this%20email.">
        <i class="icon icon-envelope" aria-hidden="true"></i>Request docs changes
    </a>
    <a href="#open-modal-pdf">
        <i class="icon icon-pdf" aria-hidden="true"></i>Download PDF
    </a>
</div>
              
                <p class="wy-menu-secondary-header">On this page</p>
                <ul>
<li><a class="reference internal" href="#">Azure Databricks OCF Connector: Query Log Ingestion</a><ul>
<li><a class="reference internal" href="#prerequisite">Prerequisite</a><ul>
<li><a class="reference internal" href="#supported-azure-storage-types">Supported Azure Storage Types</a><ul>
<li><a class="reference internal" href="#azure-blob-storage-or-azure-data-lake-storage-gen-2">Azure Blob Storage or Azure Data Lake Storage Gen 2</a></li>
<li><a class="reference internal" href="#azure-data-lake-storage-gen-1">Azure Data Lake Storage Gen 1</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#enable-logging-in-databricks">Enable Logging in Databricks</a><ul>
<li><a class="reference internal" href="#enable-logging-using-a-regular-script">Enable Logging Using a Regular Script</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configure-file-based-qli">Configure File-Based QLI</a><ul>
<li><a class="reference internal" href="#configure-azure-blob-storage-connection">Configure Azure Blob Storage Connection</a></li>
<li><a class="reference internal" href="#configure-adls-connection">Configure ADLS Connection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configure-table-based-qli">Configure Table-Based QLI</a><ul>
<li><a class="reference internal" href="#configuration-in-databricks">Configuration in Databricks</a></li>
<li><a class="reference internal" href="#configuration-in-alation">Configuration in Alation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#run-qli">Run QLI</a></li>
</ul>
</li>
</ul>

              
            </div>
          </div>
      </div>
    </div>

  </div>

  
  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->

  
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/tabs.js"></script>
      <script type="text/javascript" src="../../../_static/js/rtd_sphinx_search.min.js"></script>
  
  <script type="text/javascript" src="../../../_static/searchtools.js"></script> <script type="text/javascript" id="idPiwikScriptPlaceholder"></script><script type="text/javascript" src="../../../_static/js/main.bundle.js"></script>
  <script type="text/javascript" src="../../../_static/js/runtime.bundle.js"></script>

  
  

  
    <div id="open-modal-pdf" class="modal-window">
<div>
  <a href="#" title="Close" class="modal-close">Close</a>
  <h4>Alation Documentation PDF</h4>
  <p>This PDF contains all our documentation and is more than <b>120 MB</b> in size. We recommend using an ethernet or Wi-Fi connection for downloading.</p>
  <br>
  <div class="modal-buttons">
    <a href="https://pdf-docs.alation.com/AlationUserDocs.pdf" class="btn-cta btn-cta--primary" target="_blank">
      Download PDF
    </a>
    <a href="#" title="Close" class="btn-cta">Cancel</a>
  </div>
</div>
  

</body>
</html>
<!--3.6.17-->